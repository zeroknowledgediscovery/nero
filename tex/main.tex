\PassOptionsToPackage{usenames,x11names, dvipsnames, svgnames}{xcolor}
\documentclass[9pt,twocolumn,oneside]{pnas-new}
% Use the lineno option to display guide line numbers if required.
\setboolean{displaywatermark}{false}
\usepackage{orcidlink}   % <— add this

% \input{preamble_ieee.tex}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
%\usepackage{pdfpages}
%\usepackage{shellesc}
%\ShellEscape{pdflatex si}
\usetikzlibrary{shapes,calc,shadows,fadings,arrows,decorations.pathreplacing,automata,positioning}
\usepackage{hyperref}
\usetikzlibrary{external}
\tikzexternalize[prefix=./Figures/External/]% activate
\tikzexternaldisable 
\usetikzlibrary{decorations.text}
\usepgfplotslibrary{colorbrewer} 
\usepgfplotslibrary{statistics}
\usepgfplotslibrary{fillbetween}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[algo2e,ruled,vlined]{algorithm2e}

\usepackage{mathtools}
\usepackage{amssymb,amsfonts,amsmath,amsthm}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma} 
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{exmpl}{Example}
\newtheorem{rem}{Remark}
\newtheorem{notn}{Notation}
\usepackage{xspace}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}%

\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\bracket}[1]{\left[ #1 \right]}
% \newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\nrm}[1]{\left\llbracket{#1}\right\rrbracket}
\newcommand{\parenBar}[2]{\paren{#1\,{\left\Vert\,#2\right.}}}
\newcommand{\parenBarl}[2]{\paren{\left.#1\,\right\Vert\,#2}}
\newcommand{\ie}{$i.e.$\xspace}
\newcommand{\addcitation}{\textcolor{black!50!red}{\textbf{ADD CITATION}}}
\newcommand{\subtochange}[1]{{\color{black!50!green}{#1}}}
\newcommand{\tobecompleted}{{\color{black!50!red}TO BE COMPLETED.}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\expect}{\mathbf{E}}
\DeclareMathOperator*{\var}{\mathbf{Var}}

\newcommand{\pIn}{\mathscr{P}_{\textrm{in}}}
\newcommand{\pOut}{\mathscr{P}_{\textrm{out}}}
\newcommand{\aIn}[1][\Sigma]{#1_{\textrm{in}}}
\newcommand{\aOut}[1][\Sigma]{#1_{\textrm{out}}}
\newcommand{\xin}[1]{#1_{\textrm{in}}}
\newcommand{\xout}[1]{#1_{\textrm{out}}}

\newcommand{\R}{\mathbb{R}} % Set of real numbers
\newcommand{\F}[1][]{\mathcal{F}_{#1}}
\newcommand{\SR}{\mathcal{S}} % Semiring of sets
\newcommand{\RR}{\mathcal{R}} % Ring of sets
\newcommand{\N}{\mathbb{N}} % Set of natural numbers (0 included)

\newcommand{\pitilde}{\widetilde{\pi}}
\newcommand{\Pitilde}{\widetilde{\Pi}}

\newcommand{\Pp}[1][n]{\mathscr{P}^+_{#1}}
%% ## Equation Space Control---------------------------
\def\EQSP{3pt}
\newcommand{\mltlne}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{equation}\begin{multlined} #2 \end{multlined}\end{equation}\endgroup\noindent}
\newcommand{\cgather}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{gather} #2 \end{gather}\endgroup\noindent}
\newcommand{\cgathers}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{gather*} #2 \end{gather*}\endgroup\noindent}
\newcommand{\calign}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{align} #2 \end{align}\endgroup\noindent}
\newcommand{\caligns}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{align*} #2 \end{align*}\endgroup\noindent}
\newcommand{\mnp}[2]{\begin{minipage}{#1}#2\end{minipage}} 
\newif\iftikzX
\tikzXtrue
\tikzXfalse 
\newif\ifFIGS  
\FIGSfalse  
\FIGStrue
\cfoot{\scriptsize\thepage}
\lfoot{} 
\rfoot{} 
\def\EXTENDEDDATA{Extended Data\xspace}
\def\SUPPLEMENTARY{Supplementary\xspace}
\def\Methods{Methods\xspace}
\newcounter{Dcounter}
\setcounter{Dcounter}{1}
\newcommand{\DQS}[1]{
    \marginpar{
      \tikzexternaldisable 
      \tikz{
        \node[
          rounded corners=5pt,
          draw=none,
          thick,
          fill=black!10,
          font=\sffamily\fontsize{7}{8}\selectfont
        ]{\mnp{.3in} {\color{Red3}\raggedright  \#\theDcounter.~#1}}; 
      }
    }
  \stepcounter{Dcounter}\xspace
}


\templatetype{pnasbriefreport} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission
\def\TITLE{Complexity Signature of Generated Text}%Model-agnostic  Recognition of AI-Generated Text with Descriptional Complexity Measure}
\title{\TITLE}
    \def\NERO{NERO\xspace}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a]{Ross Schmidt}
%\author[b,1,2]{Author Two}
\author[a]{Ishanu Chattopadhyay\orcidlink{0000-0001-8339-8162}}

\affil[a]{University of Kentucky}
%\affil[b]{Affiliation Two}
%\affil[c]{Affiliation Three}

% Please give the surname of the lead author for the running footer
\leadauthor{Chattopadhyay}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{RS carried out experimental runs and wrote the paper, IC conceived of research, implemented the algorithm, wrote the paper and procured suppport.}
\authordeclaration{Authors have no competing interests.}
%\equalauthors{\textsuperscript{1}A.O.(Ross) contributed equally to this work with A.T. (Author Two) (remove if not applicable).}
\correspondingauthor{\textsuperscript{2}To whom correspondence should be addressed. E-mail: ishanu\_ch@uky.edu}

% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol
\keywords{entropy rate $|$ AI-generated text detection   $|$ large language models  $|$  algorithmic complexity  $|$ probabilistic  automata }
%% \keywords{entropy rate AI-generated text detection large language models $|$        algorithmic complexity $|$ probabilistic finite-state automata}

\begin{abstract}
We introduce a model-agnostic  approach for detecting LLM-generated text by estimating the entropy rate of text mapped to symbol streams over a (26+space)=27-letter alphabet, and   show that long-form outputs from  LLMs exhibit systematically lower entropy rates than human-authored prose. Our Nonparametric Entropy-Rate Oracle (\NERO) exploits this signal to achieve competitive, training-free separation of human and AI text, without requiring model access. When substring-frequency cutoff profile for the estimator is  used as input to a  supervised Gaussian process classifier, \NERO achieves near-perfect detection accuracy (AUC = $98.9\%$), providing a principled, complexity-theoretic framework for ranking generative capacity and tracking changes in LLM behavior over time.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}  

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\firstpage{5} 
% Use \firstpage to indicate which paragraph and line will start the second page and subsequent formatting. In this example, there are a total of 11 paragraphs on the first page, counting the first level heading as a paragraph. The value {12} represents the number of the paragraph starting the second page. If a paragraph runs over onto the second page, include a bracket with the paragraph line number starting the second page, followed by the paragraph number in curly brackets, e.g. "\firstpage[4]{11}".


% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentati

\dropcap{W}ith the rise of generative artificial intelligence (AI), particularly large language models (LLMs), reliably distinguishing human-authored prose from machine-generated text is no longer trivial~\cite{naturebe2023prepare,crothers2023machine,thirunavukarasu2023large}. Here we test the hypothesis that long-form human- and machine-generated text differ in statistical complexity when mapped to symbol streams over a 27-character alphabet (26 letters plus space) and treated as sample paths of a stochastic process~\cite{CoverKing1978}, whose entropy rate quantifies intrinsic complexity. Most existing detectors~\cite{detectgpt_icml2023,detectllm_findings2023} instead rely on language-model scoring (e.g., likelihood/rank statistics), supervised classifiers over stylistic representations, or compressibility proxies, and therefore depend on reference model access, training, or calibration to evolving generators.

We introduce a nonparametric, learning-free entropy-rate estimator that operates directly on text, without model access, supervision, or retraining. 
%
Used as a threshold score, the resulting Nonparametric Entropy-Rate Oracle (\NERO) achieves competitive,  training-free discrimination, and reveals systematically lower entropy rates in contemporary LLM outputs than in human prose under a shared symbolization. Optionally, using the estimator’s internal substring-frequency cutoff profile as features in a Gaussian process classifier yields near-perfect performance (AUC $=98.9\%$), surpassing model-driven baselines.
%
Thus, \NERO-estimated entropy rate functions as a model-agnostic statistic, akin to a physical quantity, for  effective generative capacity, and provides a robust  mechanism for AI text detection and a principled framework for ranking and tracking generative models over time. Since we target an intrinsic property of long-form text,   task-based LLM benchmarks are not directly commensurate here (see Supplementary Information).

\section*{Results}
To  connect generative capacity with algorithmic complexity and then to entropy-rate of outputs, we begin by recalling the foundational concept of optimal two-part codes in algorithmic information theory~\cite{GacsTrompVitanyi2001}. The Kolmogorov complexity \(K(x)\) of a string \(x\) is defined as the length of the shortest program (in a fixed universal programming language) that produces \(x\) and halts. It provides a rigorous, machine-independent measure of the information content or compressibility of a string. Instead of describing \(x\) directly, we can choose to first describe a finite set \(S\) that contains \(x\), and then identify  \(x\) within \(S\) by its index in a standard enumeration of all items in $S$. This leads to the two-part code representation:
\cgather{
K(x) \leq K(S) + \log |S| + O(1),
}
where \(K(S)\) is the complexity of describing the set \(S\), and \(\log |S|\) is the number of bits required to identify \(x\) within \(S\). When \(x\) is a \emph{typical} element of \(S\), $i.e.$,  that it does not admit any significantly shorter description than most elements of \(S\), the inequality becomes tight up to additive constants. The optimal two-part code is obtained by minimizing the sum \(K(S) + \log |S|\) over all such sets containing \(x\), yielding a minimal sufficient statistic for the data.

Now lets apply these notions to quantify the generative ability of  AI agents (and humans). With each  agent \(G_i\) we associate a set \(S_i\) that defines the collection of strings that the agent can possibly generate. Once  \(S_i\) is fixed, we assume that the agent produces an individual output \(x \in S_i\) using a standard  sampling procedure. This shared decoding mechanism implies that the conditional complexity \(K(x \mid S_i)\) is approximately constant across agents for typical strings. An equivalent way to view this assumption, is that   agents draw from the same or similar set of possible  strings, $i.e.$, if one agent can generate a particular string \(x\), then so can the other, perhaps with different odds. Under either interpretation, the implication is that the conditional term \(K(x \mid S_i)\) is effectively constant across agents, and since for typical \(x\) this term satisfies \(K(x \mid S_i) \approx \log |S_i|\)~\cite{GacsTrompVitanyi2001}, we have:
\cgather{\label{eq71}
\log |S_1| = \log |S_2| + O(1).
}%
Then, we have the proposition (See \Methods for proof):
%
\begin{prop}[Two-Part Code to  Entropy]\label{prop1}
Let \(G_1\) and \(G_2\) be two generative processes outputting strings over a finite alphabet \(x \in \mathcal{A}^n\), each respectively associated with a set \(S_1\) and \(S_2\) of possible outputs. Assuming
$\log |S_1| = \log |S_2| + O(1)$,  the following equivalence holds:
\mltlne{
K(S_1) > K(S_2) + O(1)
\Leftrightarrow
\mathop{E}_{x \sim G_1}[K(x)] > \mathop{E}_{x \sim G_2}[K(x)] + O(1). \notag
}
Additionally, if $G_i$ are stationary,%% if the output strings \(x \sim G_i\) are drawn from stationary sources,
then
\mltlne{
K(S_1) > K(S_2) + O(1) \Longrightarrow
H(G_1) > H(G_2) + o(1),
}%
where \(H(G_i)\) is the Shannon entropy rate of generator \(G_i\).
\end{prop}
Proposition~\ref{prop1} motivates entropy rate as a discriminative statistic, suggesting recognition of AI-generated text by its systematically lower entropy rate. Estimating entropy rate for symbol streams is nontrivial~\cite{grassberger1989estimating}. Accordingly, for \NERO we adopt our prior nonparametric entropy-rate estimators based on probabilistic finite-state automata (PFSA)~\cite{CLx,CL12g} (See Supplementary \Methods). With the  standard approximate stationarity assumption~\cite{CoverKing1978} for long-form texts, Proposition~\ref{prop1} provides the key foundation of our claim.

%#######################################################
\begin{table}[t]
\centering
\caption{Cohorts and average entropy rates}\label{tab1}
\input{Figures/corpus.tex}
\end{table}
\begin{figure}[!t]
\centering
  
\vspace{0pt}

\includegraphics[width=.45\textwidth]{Figures/External/perfnew}

\vspace{-7pt}
 
 \caption{\textbf{\NERO\ performance.} 
\textbf{a}, Pooled human-AI ROC curves for training-free \NERO  (median entropy-rate estimate $\hat H$), and with a Gaussian-process classifier alongside  baselines (perplexity from \textsc{HowGPT} and  \textsc{ZeroGPT} scoring, See Supplementary Methods).
\textbf{b}, Cross-cohort robustness across 381 out-of-sample discrimination tasks obtained by varying the human reference set (Gutenberg, arXiv, or both) and the subset of LLM generators.
\textbf{c}, Mean entropy rates plotted at model release dates (“birthtime”), illustrating upward drift toward the human regime; a weighted Richards fit with a fixed ceiling at the mean human rate is overlaid for descriptive smoothing.
\textbf{d}, Entropy-rate estimates versus document length under  truncation (using Gutenberg texts for humans), indicating a practical lower bound for reliable discrimination at shorter lengths around $5,000$ characters.
\textbf{e}, Genre-stratified \NERO entropy rates for human prose; error bars denote 95\% intervals.
}\label{figperf}
  \end{figure}     
\section*{Experiments}\label{sec:experiments}
To evaluate our claim, we use a  corpus (See Table~\ref{tab1}), comprising human-authored texts (Project Gutenberg and arXiv) and long-form outputs from  contemporary LLMs (GPT-3.5, GPT-4o, GPT-4.0, Claude, Gemini,
and GPT-5), using a mix of API and web-form access (See Supplementary Methods). All documents were lowercased and mapped to a 27-symbol
English-plus-space alphabet, removing punctuation, digits, and non-ASCII
characters. For each document we compute a family of entropy-rate estimates
across substring-frequency thresholds $\{m_1,\dots,m_M\}$, where $m$
specifies the minimum number of occurrences required for a substring to
be retained in the PFSA construction (see \Methods). In the training-free
setting, we use the median across thresholded $m$-estimates ($H$). In the trained-\NERO setting, the full $m$-dependence profile is used as features for training a Gaussian process classifier~\cite{williams2006gaussian}; importantly, the underlying entropy-rate estimator is unchanged. Using \NERO, our  estimates for arxiv papers ($\approx 1.3$ bits/letter, Table~\ref{tab1}) closely align with classic estimates of English entropy under a 27-symbol alphabet~\cite{CoverKing1978}, whereas Gutenberg prose exhibits  lower character-level entropy, consistent with greater redundancy in narrative text. Our experimental results (Fig.~\ref{figperf}) demonstrate:

(1) \emph{Training-free detection.} Using $H$ directly as a detection score yields strong LLM-human discrimination without any training. (AUC $=0.824$ on the pooled evaluation, Fig.~\ref{figperf}a).

(2) \emph{Training using substring-frequency threshold $m$-dependence.}
We trained a Gaussian-process classifier on the $m$-dependent feature vector
(see \Methods) using a $50/50$ train--test split, improving 
out-of-sample discrimination to AUC $=0.989$ (Fig.~\ref{figperf}a). For
comparison, the perplexity baseline (HowGPT) attains AUC $=0.830$, while
\textsc{ZeroGPT} attains AUC $=0.946$ on the same pooled evaluation.

(3) \emph{Cross-cohort robustness.} Fig.~\ref{figperf}b reports out-of-sample AUCs for $(2^2-1)\times(2^7-1)=381$ distinct human-AI discrimination task obtained by varying the human reference and the subset of LLM generators included (See SI Methods). Across this combinatorial suite, training-free \NERO maintains consistently strong performance, whereas trained baseline detectors exhibit substantially greater sensitivity to corpus composition and generator selection. This contrast underscores the \NERO estimate as a calibration-free, model-agnostic statistic rather than a generator-dependent detector.

(4) \emph{Temporal trajectory toward the human regime.} Fig.~\ref{figperf}c plots cohort-level mean \NERO-computed entropy rates at model release times with a weighted Richards (generalized logistic) fit and a fixed asymptotic ceiling given by the mean human rate (SI Methods), showing successive model generations shifting upward toward the human regime with diminishing returns. While not a calendar-date forecast, this supports the qualitative conclusion that model improvements are narrowing the entropy-rate gap. GPT-4.0 is a notable outlier, consistent with a regime shift induced by the architectural jump introduced in this model (multi-modal access and other experimental features)~\cite{openai_gpt4_architecture}.

(5) \emph{Length dependence.} Human-AI separation is robust for articles with $> 10,000$ characters (about the length of a full-length magazine article), with loss of discrimination possibly at  lengths under $5,000$ characters (Fig.~\ref{figperf}d). 

(6) \emph{Genre dependence.}  Formulaic human genres (Mathematical text, Children's Literature) attain lower entropy rates than expository genres ($e.g.$ Literary criticism and political theory),
defining the primary hard cases (Fig.~\ref{figperf}e).

\section*{Discussion}
We show that, under  shared symbolization, long-form human-authored text exhibits a consistently higher estimated entropy rate than contemporary LLMs. In the two-part code perspective, when outputs are compared within a common representation, differences in entropy rate are consistent with differences in the effective descriptive complexity of the underlying generative mechanisms, suggesting that  humans might operate with a richer, more complex internal model than current  AI systems.%, which are ultimately  constrained by their training data.

A complementary intuition follows from \emph{Levin’s universal semimeasure} $m(x)$~\cite{Levin1974}, which lower-bounds the probability assignable to strings by any computable process. It defines a universal prior by summing over all prefix-free programs $p$~\cite{LiVitanyi2008} that produce $x$ on a universal Turing machine $U$~\cite{Chaitin1975}:
\cgather{
m(x) = \sum_{p : U(p) = x} 2^{-|p|}.
}
and Levin’s Coding Theorem links  to Kolmogorov complexity,
\cgather{
K(x) = -\log m(x) + O(1),
}
which implies that strings with low \(m(x)\) are algorithmically complex and lie in the deep tails of any computable generative process. Humans operating with an internal generative model \(S_{\text{human}}\) that likely possesses significantly higher descriptive complexity than models used in AI systems, are able to generate outputs that lie deeper in the algorithmic tail of \(m(x)\), $i.e.$, outputs that are rarer, more contextually novel, and of higher complexity. 
%
In contrast, LLMs are trained on finite, empirical datasets where such rare strings are underrepresented (by definition of being rare); since strings with low \(m(x)\) are, by definition, uncommon, they are statistically unlikely to appear with sufficient frequency in training corpora. This possibly limits the effective model complexity \(K(S_{\text{AI}})\), and hence constrains the AI's ability to reproduce or extrapolate into these low-probability regions.


Thus, despite the limitations of  requiring longer texts and greater computational effort, \NERO yields a model-agnostic statistic that enables best-in-class detection, and principled, learning-free tracking of generative behavior over time.



\matmethods{
\paragraph{Data acquisition.} Human-authored texts were obtained from Project Gutenberg (English-language compositions) and technical manuscripts from arXiv via the official OAI-PMH interface. AI-generated texts were produced via webform and API access to the LLMs listed in Table~\ref{tab1}. Details on prompt design, API access, and \NERO computation are provided in the Supplementary Methods.

\paragraph{Code availability.} A  reproducible implementation of the entropy-rate estimator and end-to-end evaluation pipeline is available under an MIT license at \url{https://github.com/zeroknowledgediscovery/nero}.
}

\showmatmethods{} % Display the Materials and Methods section
\acknow{This work was supported in part by the Defense Advanced Research Projects Agency (DARPA) under the ARC program (HR0011-26-3-E016). Views, opinions, and findings expressed are solely those of the authors.}

\showacknow{} % Display the acknowledgments section

%##################################################
% #######################################
%#################################################
%#################################################


\bibliography{BibLib1,tom}

\include{SIpnas.tex}
 
\end{document}
