%% AIs:
%% Claude
%% Gemini (deep mind)


%% Detectors:
%% ghostbuster:
%% https://github.com/vivek3141/ghostbuster

%% DetectGPT
%% https://detectgpt.com/app/

%% GPTzero

%% Originality.AI

%% Roberta


%% We need at least 10K cohort each in AI and human text.
%% Anything les will not be serious.

%% We need a figure which shows the different models ranked in ordrer of their H number, x axis can be time to show progression.

%% our key positions here is that we are not competingto be the best detector. But providing theoretical insight into what distinguishes AI text from human text and training free future proof ranking ability



\PassOptionsToPackage{usenames,x11names, dvipsnames, svgnames}{xcolor}
\documentclass[9pt,twocolumn,oneside]{pnas-new}
% Use the lineno option to display guide line numbers if required.
\setboolean{displaywatermark}{false}
\usepackage{orcidlink}   % <— add this

% \input{preamble_ieee.tex}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
%\usepackage{pdfpages}
%\usepackage{shellesc}
%\ShellEscape{pdflatex si}
\usetikzlibrary{shapes,calc,shadows,fadings,arrows,decorations.pathreplacing,automata,positioning}
\usepackage{hyperref}
\usetikzlibrary{external}
\tikzexternalize[prefix=./Figures/External/]% activate
\tikzexternaldisable 
\usetikzlibrary{decorations.text}
\usepgfplotslibrary{colorbrewer} 
\usepgfplotslibrary{statistics}
\usepgfplotslibrary{fillbetween}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[algo2e,ruled,vlined]{algorithm2e}

\usepackage{mathtools}
\usepackage{amssymb,amsfonts,amsmath,amsthm}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma} 
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{exmpl}{Example}
\newtheorem{rem}{Remark}
\newtheorem{notn}{Notation}
\usepackage{xspace}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}%

\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\bracket}[1]{\left[ #1 \right]}
% \newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\nrm}[1]{\left\llbracket{#1}\right\rrbracket}
\newcommand{\parenBar}[2]{\paren{#1\,{\left\Vert\,#2\right.}}}
\newcommand{\parenBarl}[2]{\paren{\left.#1\,\right\Vert\,#2}}
\newcommand{\ie}{$i.e.$\xspace}
\newcommand{\addcitation}{\textcolor{black!50!red}{\textbf{ADD CITATION}}}
\newcommand{\subtochange}[1]{{\color{black!50!green}{#1}}}
\newcommand{\tobecompleted}{{\color{black!50!red}TO BE COMPLETED.}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\expect}{\mathbf{E}}
\DeclareMathOperator*{\var}{\mathbf{Var}}

\newcommand{\pIn}{\mathscr{P}_{\textrm{in}}}
\newcommand{\pOut}{\mathscr{P}_{\textrm{out}}}
\newcommand{\aIn}[1][\Sigma]{#1_{\textrm{in}}}
\newcommand{\aOut}[1][\Sigma]{#1_{\textrm{out}}}
\newcommand{\xin}[1]{#1_{\textrm{in}}}
\newcommand{\xout}[1]{#1_{\textrm{out}}}

\newcommand{\R}{\mathbb{R}} % Set of real numbers
\newcommand{\F}[1][]{\mathcal{F}_{#1}}
\newcommand{\SR}{\mathcal{S}} % Semiring of sets
\newcommand{\RR}{\mathcal{R}} % Ring of sets
\newcommand{\N}{\mathbb{N}} % Set of natural numbers (0 included)

\newcommand{\pitilde}{\widetilde{\pi}}
\newcommand{\Pitilde}{\widetilde{\Pi}}

\newcommand{\Pp}[1][n]{\mathscr{P}^+_{#1}}
%% ## Equation Space Control---------------------------
\def\EQSP{3pt}
\newcommand{\mltlne}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{equation}\begin{multlined} #2 \end{multlined}\end{equation}\endgroup\noindent}
\newcommand{\cgather}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{gather} #2 \end{gather}\endgroup\noindent}
\newcommand{\cgathers}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{gather*} #2 \end{gather*}\endgroup\noindent}
\newcommand{\calign}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{align} #2 \end{align}\endgroup\noindent}
\newcommand{\caligns}[2][\EQSP]{\begingroup\setlength\abovedisplayskip{#1}\setlength\belowdisplayskip{#1}\begin{align*} #2 \end{align*}\endgroup\noindent}
\newcommand{\mnp}[2]{\begin{minipage}{#1}#2\end{minipage}} 
\newif\iftikzX
\tikzXtrue
\tikzXfalse 
\newif\ifFIGS  
\FIGSfalse  
\FIGStrue
\cfoot{\scriptsize\thepage}
\lfoot{} 
\rfoot{} 
\def\EXTENDEDDATA{Extended Data\xspace}
\def\SUPPLEMENTARY{Supplementary\xspace}
\def\Methods{Methods\xspace}
\newcounter{Dcounter}
\setcounter{Dcounter}{1}
\newcommand{\DQS}[1]{
    \marginpar{
      \tikzexternaldisable 
      \tikz{
        \node[
          rounded corners=5pt,
          draw=none,
          thick,
          fill=black!10,
          font=\sffamily\fontsize{7}{8}\selectfont
        ]{\mnp{.3in} {\color{Red3}\raggedright  \#\theDcounter.~#1}}; 
      }
    }
  \stepcounter{Dcounter}\xspace
}




\templatetype{pnasbriefreport} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission
\def\TITLE{Model-agnostic  Recognition of AI-Generated Text with Descriptional Complexity Measure}
\title{\TITLE}
    \def\NERO{NERO\xspace}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a]{Ross Schmidt}
%\author[b,1,2]{Author Two}
\author[a]{Ishanu Chattopadhyay\orcidlink{0000-0001-8339-8162}}

\affil[a]{University of Kentucky}
%\affil[b]{Affiliation Two}
%\affil[c]{Affiliation Three}

% Please give the surname of the lead author for the running footer
\leadauthor{Chattopadhyay}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{RS carried out experimental runs and wrote the paper, IC conceived of research, implemented the algorithm, wrote the paper and procured suppport.}
\authordeclaration{Authors have no competing interests.}
%\equalauthors{\textsuperscript{1}A.O.(Ross) contributed equally to this work with A.T. (Author Two) (remove if not applicable).}
\correspondingauthor{\textsuperscript{2}To whom correspondence should be addressed. E-mail: ishanu\_ch@uky.edu}

% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol
\keywords{entropy rate $|$ AI-generated text detection   $|$ large language models  $|$  algorithmic complexity  $|$ probabilistic  automata }
%% \keywords{entropy rate AI-generated text detection large language models $|$        algorithmic complexity $|$ probabilistic finite-state automata}

\begin{abstract}
We introduce a model-agnostic  approach for detecting LLM-generated text by estimating the entropy rate of text mapped to symbol streams over a (26+space)=27-letter alphabet, and   show that long-form outputs from  LLMs exhibit systematically lower entropy rates than human-authored prose. Our Nonparametric Entropy-Rate Oracle (\NERO) exploits this signal to achieve competitive, training-free separation of human and AI text, without requiring model access. When substring-frequency cutoff profile for the estimator is  used as input to a  supervised Gaussian process classifier, \NERO achieves near-perfect detection accuracy (AUC = $98.9\%$), providing a principled, complexity-theoretic framework for ranking generative capacity and tracking changes in LLM behavior over time.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

%#todo
% compare auc with 3.5 with entropy and zerogpt
% compare with short lengths
% do training

\begin{document}  

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\firstpage{5} 
% Use \firstpage to indicate which paragraph and line will start the second page and subsequent formatting. In this example, there are a total of 11 paragraphs on the first page, counting the first level heading as a paragraph. The value {12} represents the number of the paragraph starting the second page. If a paragraph runs over onto the second page, include a bracket with the paragraph line number starting the second page, followed by the paragraph number in curly brackets, e.g. "\firstpage[4]{11}".


% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentati


\dropcap{W}ith the rise of generative artificial intelligence (AI), particularly large language models (LLMs), reliably distinguishing human-authored prose from machine-generated text is no longer trivial~\cite{naturebe2023prepare,crothers2023machine,thirunavukarasu2023large}. Here we test the hypothesis that long-form human- and machine-generated text differ in statistical complexity when mapped to symbol streams over a 27-character alphabet (26 letters plus space) and treated as sample paths of a stochastic process~\cite{CoverKing1978}, whose entropy rate quantifies intrinsic complexity. Most existing detectors~\cite{detectgpt_icml2023,detectllm_findings2023} instead rely on language-model scoring (e.g., likelihood/rank statistics), supervised classifiers over stylistic representations, or compressibility proxies, and therefore depend on reference model access, training, or calibration to evolving generators.

We introduce a nonparametric, learning-free entropy-rate estimator that operates directly on text, without model access, supervision, or retraining. Used as a threshold score, the resulting Nonparametric Entropy-Rate Oracle (\NERO) achieves competitive,  training-free discrimination, and reveals systematically lower entropy rates in contemporary LLM outputs than in human prose under a shared symbolization. Optionally, using the estimator’s internal substring-frequency cutoff profile as features in a Gaussian process classifier yields near-perfect performance (AUC $=98.9\%$), surpassing model-driven baselines.
%
Thus, \NERO-estimated entropy rate functions as a model-agnostic statistic, akin to a physical quantity, for  effective generative capacity, and provides a robust  mechanism for AI text detection and a principled framework for ranking and tracking generative models over time.

\section*{Results}
To  connect generative capacity with algorithmic complexity and then to entropy-rate of outputs, we begin by recalling the foundational concept of optimal two-part codes in algorithmic information theory~\cite{GacsTrompVitanyi2001}. The Kolmogorov complexity \(K(x)\) of a string \(x\) is defined as the length of the shortest program (in a fixed universal programming language) that produces \(x\) and halts. It provides a rigorous, machine-independent measure of the information content or compressibility of a string. Instead of describing \(x\) directly, we can choose to first describe a finite set \(S\) that contains \(x\), and then identify  \(x\) within \(S\) by its index in a standard enumeration of all items in $S$. This leads to the two-part code representation:
\cgather{
K(x) \leq K(S) + \log |S| + O(1),
}
where \(K(S)\) is the complexity of describing the set \(S\), and \(\log |S|\) is the number of bits required to identify \(x\) within \(S\). When \(x\) is a \emph{typical} element of \(S\), $i.e.$,  that it does not admit any significantly shorter description than most elements of \(S\), the inequality becomes tight up to additive constants. The optimal two-part code is obtained by minimizing the sum \(K(S) + \log |S|\) over all such sets containing \(x\), yielding a minimal sufficient statistic for the data.

Now lets apply these notions to quantify the generative ability of  AI agents (and humans). With each  agent \(G_i\) we associate a set \(S_i\) that defines the collection of strings that the agent can possibly generate. Once  \(S_i\) is fixed, we assume that the agent produces an individual output \(x \in S_i\) using a standard  sampling procedure. This shared decoding mechanism implies that the conditional complexity \(K(x \mid S_i)\) is approximately constant across agents for typical strings. An equivalent way to view this assumption, is that   agents draw from the same or similar set of possible  strings, $i.e.$, if one agent can generate a particular string \(x\), then so can the other, perhaps with different odds. Under either interpretation, the implication is that the conditional term \(K(x \mid S_i)\) is effectively constant across agents, and since for typical \(x\) this term satisfies \(K(x \mid S_i) \approx \log |S_i|\)~\cite{GacsTrompVitanyi2001}, we have:
\cgather{\label{eq71}
\log |S_1| = \log |S_2| + O(1).
}%
Then, we have the proposition (See \Methods for proof):
%
\begin{prop}[Two-Part Code to  Entropy]\label{prop1}
Let \(G_1\) and \(G_2\) be two generative processes outputting strings over a finite alphabet \(x \in \mathcal{A}^n\), each respectively associated with a set \(S_1\) and \(S_2\) of possible outputs. Assuming
$\log |S_1| = \log |S_2| + O(1)$,  the following equivalence holds:
\mltlne{
K(S_1) > K(S_2) + O(1)
\Leftrightarrow
\mathop{E}_{x \sim G_1}[K(x)] > \mathop{E}_{x \sim G_2}[K(x)] + O(1). \notag
}
Additionally, if $G_i$ are stationary,%% if the output strings \(x \sim G_i\) are drawn from stationary sources,
then
\mltlne{
K(S_1) > K(S_2) + O(1) \Longrightarrow
H(G_1) > H(G_2) + o(1),
}%
where \(H(G_i)\) is the Shannon entropy rate of generator \(G_i\).
\end{prop}
Proposition~\ref{prop1} motivates entropy rate as a discriminative statistic, suggesting recognition of AI-generated text by its systematically lower entropy rate. Estimating entropy rate for symbol streams is nontrivial~\cite{grassberger1989estimating}. Accordingly, for \NERO we adopt our prior nonparametric entropy-rate estimators based on probabilistic finite-state automata (PFSA)~\cite{CLx,CL12g} (See Supplementary \Methods). With the  standard approximate stationarity assumption~\cite{CoverKing1978} for long-form texts, Proposition~\ref{prop1} provides the key foundation of our claim.

%#######################################################
\begin{table}[t]
\centering
\caption{Cohorts and average entropy rates}\label{tab1}
\input{Figures/corpus.tex}
\end{table}
\begin{figure}[!t]
\centering
  
\vspace{0pt}

\includegraphics[width=.45\textwidth]{Figures/External/perfnew}

\vspace{-7pt}
 
 \caption{\textbf{\NERO\ performance.} 
\textbf{a}, Pooled human-AI ROC curves for training-free \NERO  (median entropy-rate estimate $\hat H$), and with a Gaussian-process classifier alongside  baselines (perplexity from \textsc{HowGPT} and  \textsc{ZeroGPT} scoring, See Supplementary Methods).
\textbf{b}, Cross-cohort robustness across 381 out-of-sample discrimination tasks obtained by varying the human reference set (Gutenberg, arXiv, or both) and the subset of LLM generators.
\textbf{c}, Mean entropy rates plotted at model release dates (“birthtime”), illustrating upward drift toward the human regime; a weighted Richards fit with a fixed ceiling at the mean human rate is overlaid for descriptive smoothing.
\textbf{d}, Entropy-rate estimates versus document length under  truncation (using Gutenberg texts for humans), indicating a practical lower bound for reliable discrimination at shorter lengths around $10,000$ characters.
\textbf{e}, Genre-stratified \NERO entropy rates for human prose; error bars denote 95\% intervals.
}\label{figperf}
  \end{figure}     
\section*{Experiments}\label{sec:experiments}
To evaluate our claim, we use a  corpus (See Table~\ref{tab1}), comprising human-authored texts (Project Gutenberg and arXiv) and long-form outputs from  contemporary LLMs (GPT-3.5, GPT-4o, GPT-4.0, Claude, Gemini,
and GPT-5), using a mix of API and web-form access (See Supplementary Methods). All documents were lowercased and mapped to a 27-symbol
English-plus-space alphabet, removing punctuation, digits, and non-ASCII
characters. For each document we compute a family of entropy-rate estimates
across substring-frequency thresholds $\{m_1,\dots,m_M\}$, where $m$
specifies the minimum number of occurrences required for a substring to
be retained in the PFSA construction (see \Methods). In the training-free
setting, we use the median across thresholded $m$-estimates ($H$). In the trained-\NERO setting, the full $m$-dependence profile is used as features for training a Gaussian process classifier~\cite{williams2006gaussian}; importantly, the underlying entropy-rate estimator is unchanged. Using \NERO, our  estimates for arxiv papers ($\approx 1.3$ bits/letter, Table~\ref{tab1}) closely align with classic estimates of English entropy under a 27-symbol alphabet~\cite{CoverKing1978}, whereas Gutenberg prose exhibits  lower character-level entropy, consistent with greater redundancy in narrative text. Our experimental results (Fig.~\ref{figperf}) demonstrate:

(1) \emph{Training-free detection.} Using $H$ directly as a detection score yields strong LLM-human discrimination without any training. (AUC $=0.824$ on the pooled evaluation, Fig.~\ref{figperf}a).

(2) \emph{Training using substring-frequency threshold $m$-dependence.}
We trained a Gaussian-process classifier on the $m$-dependent feature vector
(see \Methods) using a $50/50$ train--test split, improving 
out-of-sample discrimination to AUC $=0.989$ (Fig.~\ref{figperf}a). For
comparison, the perplexity baseline (HowGPT) attains AUC $=0.830$, while
\textsc{ZeroGPT} attains AUC $=0.946$ on the same pooled evaluation.

(3) \emph{Cross-cohort robustness.} Fig.~\ref{figperf}b reports out-of-sample AUCs for $(2^2-1)\times(2^7-1)=381$ distinct human-AI discrimination task obtained by varying the human reference and the subset of LLM generators included (See SI Methods). Across this combinatorial suite, training-free \NERO maintains consistently strong performance, whereas trained baseline detectors exhibit substantially greater sensitivity to corpus composition and generator selection. This contrast underscores the \NERO estimate as a calibration-free, model-agnostic statistic rather than a generator-dependent detector.

(4) \emph{Temporal trajectory toward the human regime.} Fig.~\ref{figperf}c plots cohort-level mean \NERO-computed entropy rates at model release times with a weighted Richards (generalized logistic) fit and a fixed asymptotic ceiling given by the mean human rate (SI Methods), showing successive model generations shifting upward toward the human regime with diminishing returns. While not a calendar-date forecast, this supports the qualitative conclusion that model improvements are narrowing the entropy-rate gap. GPT-4.0 is a notable outlier, consistent with a regime shift induced by the architectural jump introduced in this model (multi-modal access and other experimental features)~\cite{openai_gpt4_architecture}.

(5) \emph{Length dependence.} Human-AI separation is robust for articles with $> 10,000$ characters (about the length of a full-length magazine article), with loss of discrimination possibly at  lengths under $5,000$ characters (Fig.~\ref{figperf}d). 

(6) \emph{Genre dependence.}  Formulaic human genres (Mathematical text, Children's Literature) attain lower entropy rates than expository genres ($e.g.$ Literary criticism and political theory),
defining the primary hard cases (Fig.~\ref{figperf}e).

\section*{Discussion}
We show that, under  shared symbolization, long-form human-authored text exhibits a consistently higher estimated entropy rate than contemporary LLMs. In the two-part code perspective, when outputs are compared within a common representation, differences in entropy rate are consistent with differences in the effective descriptive complexity of the underlying generative mechanisms, suggesting that  humans might operate with a richer, more complex internal model than current  AI systems.%, which are ultimately  constrained by their training data.

A complementary intuition follows from \emph{Levin’s universal semimeasure} $m(x)$~\cite{Levin1974}, which lower-bounds the probability assignable to strings by any computable process. It defines a universal prior by summing over all prefix-free programs $p$~\cite{LiVitanyi2008} that produce $x$ on a universal Turing machine $U$~\cite{Chaitin1975}:
\cgather{
m(x) = \sum_{p : U(p) = x} 2^{-|p|}.
}
and Levin’s Coding Theorem links  to Kolmogorov complexity,
\cgather{
K(x) = -\log m(x) + O(1),
}
which implies that strings with low \(m(x)\) are algorithmically complex and lie in the deep tails of any computable generative process. Humans operating with an internal generative model \(S_{\text{human}}\) that likely possesses significantly higher descriptive complexity than models used in AI systems, are able to generate outputs that lie deeper in the algorithmic tail of \(m(x)\), $i.e.$, outputs that are rarer, more contextually novel, and of higher complexity. 
%
In contrast, LLMs are trained on finite, empirical datasets where such rare strings are underrepresented (by definition of being rare); since strings with low \(m(x)\) are, by definition, uncommon, they are statistically unlikely to appear with sufficient frequency in training corpora. This possibly limits the effective model complexity \(K(S_{\text{AI}})\), and hence constrains the AI's ability to reproduce or extrapolate into these low-probability regions.


Thus, despite the limitations of  requiring longer texts and greater computational effort, \NERO yields a model-agnostic statistic that enables best-in-class detection, and principled, learning-free tracking of generative behavior over time.



\matmethods{
\paragraph{Data acquisition.} Human-authored texts were obtained from Project Gutenberg (English-language compositions) and technical manuscripts from arXiv via the official OAI-PMH interface. AI-generated texts were produced via webform and API access to the LLMs listed in Table~\ref{tab1}. Details on prompt design, API access, and \NERO computation are provided in the Supplementary Methods.

\paragraph{Code availability.} A  reproducible implementation of the entropy-rate estimator and end-to-end evaluation pipeline is available under an MIT license at \url{https://github.com/your-repo/NERO}.
}

\showmatmethods{} % Display the Materials and Methods section
\acknow{This work was supported in part by the Defense Advanced Research Projects Agency (DARPA) under the ARC program. Views, opinions, and findings expressed are solely those of the authors.}

\showacknow{} % Display the acknowledgments section

%##################################################
% #######################################
%#################################################
%#################################################
%\fontsize{6}{8}\selectfont

%\subsection*{References}
 

%\bibliographystyle{tran}
\bibliography{BibLib1,tom}


\include{SIpnas.tex}
 

%\includepdf[pages=-]{si.pdf}
\end{document}
% For the estimation of entropy rate, we model discrete and finite-valued stationary and ergodic sources as probabilistic automata. Our automata is distinct from that of Paz~\cite{P71}, and each model in our case is an encoding of a measure defined on the space of strictly infinite strings over a finite alphabet~\cite{CLx,CL12g}. While the formalisms are  completely different, some aspects of this approach has subtle parallels to that of Rissanen's ``context algorithm''~\cite{rissanen83}; his search for contexts which yield similar probabilities of generating future symbols is analogous to our search for a synchronizing string in the input stream - a finite sequence of symbols that, once executed on a probabilistic automaton, leads to a fixed state irrespective of the initial conditions. Of course we do not know anything about the hidden model a priori; but nevertheless we establish that such a string, at least in a well-defined approximate sense, always exists and is identifiable efficiently. Finally, we show that, given such an approximate synchronizing string, we can use results from non-parametric statistics to bound the probability of error as a function of the input length.



%The entropy rate of a stationary and ergodic process converges in probability to the  per-letter Kolmogorov complexity of a single sufficiently long sample path~\cite{horibe03}. While  Kolmogorov complexity is incomputable, entropy rates  can, in principle, be estimated. Ability to quantify the complexity of a signal source, even in the average sense, can provide valuable insights into the driving dynamics; and can potentially be used as a  tool to detect dynamical anomalies without explicit knowledge of  background noise processes.

%However, source entropy rate estimation  from an observed sample path is computationally non-trivial. Even with the assumptions of ergodicity and stationarity, one cannot fruitfully apply the defining relation in Eq.\eqref{eqentropy} due to the exponential increase in the number of different words with the word-length. This is particularly important  if there are long-range dependencies in the symbol stream.  Such dependencies introduce additional long-range structure; decreasing the source entropy  in the process. In such cases  unacceptably long words or \textit{blocks} must be considered, and pre-mature truncation of the computation would lead to large errors. 

%The best known algorithms that carry out a  more efficient  computation are based on  Lempel-Ziv (LZ) source coding~\cite{LZ77,LZ78,LZ77opt}. The LZ coding algorithms are asymptotically optimal, $i.e.$ their compression rate approaches the source entropy rate for any ergodic stationary stochastic process. The key idea here is adaptive dictionary compression: parse the input string into distinct phrases, and represent them with codewords, making sure that  short codewords are assigned to common phrases. Done optimally, one ends up with a compressed string, such that the ratio of the input and output lengths approach the source entropy rate. Different variations on this idea have been reported~\cite{langdon83,grassberger89}. Techniques distinct from  LZ parsing  are also known, $e.g.$,  Rissanen~\cite{rissanen83} reported a universal compression scheme, which instead of gathering parsed segments of the input along with their occurrence counts, collects  the ``contexts'' in which each symbol of  the  input string  occurs, together with   conditional  occurrence counts.

%Importantly, a majority of the reported techniques do more than just compute the entropy rate; they are indeed full-scale data compression utilities, that produce a decodable representation of the input. Can we do better if we are only interested in the former? This paper provides an affirmative answer to this possibility.

%Secondly,  existing techniques lack convergence rate estimates; computation of error bars for reported approaches do not follow from any standard limit theorem. There is indeed no analytical way to check for the internal consistency of the estimation or its accuracy. We may observe gradual convergence to a limiting value, and this is indeed guaranteed by theory;  but  are unable to  provide uncertainty bounds on the computed  estimate with finite inputs. Typically observed slow convergence in all non-trivial scenarios, for all reported algorithms,  makes this a key issue. An empirical relationship, without proof or theoretical backing, has been suggested~\cite{shgs96}, which conjectures the $\vert s \vert$-dependence ($\vert s \vert$ being the length of the input $s$) of the estimated entropy rate $\widetilde{H}$ to follow
%\cgather{ 
%$\widetilde{H} \simeq H_{actual} + c \frac{\log \vert s \vert}{\vert s \vert^\gamma}, \textrm{where } c,\gamma \textrm{ are fit parameters}$. 
%}
%In this paper, we show that, at least with our algorithm, the convergence rate is given by $O(\log \vert s \vert / \sqrt[3]{\vert s \vert})$. This is a distribution-free result, in the sense that the asymptotic  bound does not depend on the source characteristics. In consequence, we can derive explicit uncertainty estimates at specified  confidence bounds on the estimated entropy rate for finite-length input data.
 
% Our approach is based on  modeling discrete and finite-valued stationary and ergodic sources as probabilistic automata. Our automata is distinct from that of Paz~\cite{P71}, and each model in our case is in fact an encoding of a measure defined on the space of strictly infinite strings over a finite alphabet. While the formalisms are  completely different, some aspects of this approach has subtle parallels to that of Rissanen's ``context algorithm''~\cite{rissanen83}; his search for contexts which yield similar probabilities of generating future symbols is analogous to our search for a synchronizing string in the input stream - a finite sequence of symbols that, once executed on a probabilistic automaton, leads to a fixed state irrespective of the initial conditions. Of course we do not know anything about the hidden model a priori; but nevertheless we establish that such a string, at least in a well-defined approximate sense, always exists and is identifiable efficiently. Finally, we show that, given such an approximate synchronizing string, we can use results from non-parametric statistics to bound the probability of error as a function of the input length.
% \dropcap{G}enerative artificial intelligence (AI) through large language models (LLMs), continue to demonstrate remarkable capabilities in generating text that closely mimic human writing styles, and are increasingly becoming invaluable for a wide range of applications, from automated content creation to enhancing natural language processing tasks. Currently, the capabilities of LLMs have improved to the point where determining whether a text is human-generated might no longer be trivial.\cite{naturebe2023prepare} This apparent indistinguishability of AI vs human generated content has raised significant concerns regarding the reliability and authenticity of information in the public domain~\cite{crothers2023machine,thirunavukarasu2023large}, and where  the integrity of the information is critical, such as in academic writing, journalism, and legal documentation.
  
% Human writing is inherently complex and nuanced, drawing upon the  rich tapestry of  experiences, emotions, and cognitive processes. In contrast, AI models, despite their rapidly advancing capabilities, are limited by the patterns and correlations learned from their training data. Here we consider the problem of  developing a robust mechanism for  distinguishing between human and AI-generated content. % and demonstrate that there are computable  disambiguating chracrteritics that need no learning or tuning.
% We posit that there  exists a measurable difference in the  inherent complexity and unpredictability of human vs AI generated text. A long-form text, when represented as a symbol stream over a 27 letter alphabet (26 letter + space), may be viewed as a  sample path from an underlying   stochastic process, and a fundamental measure of the inherent complexity of such a stochastic  generator is the  entropy rate of the  process. Here we demonstrate that our intuition that AI-generated text is recognizably less complex, leads to a practically implementable algorithm, revealing that AI achieves significantly lower entropy rate compared to human content. This insight not only leads to a robust tool for recognizing AI generated long-form text, but  emerges as the yard-stick by which to measure the quality of AI content-generation capability as LLMs continue to improve.

% Our key technical contribution here is formulating a reliable approach for estimating the entropy rate of a stochastic process, which is not necessarily i.i.d. or has some other a priori assumed simplified structure such as exchangeability, and is thus applicable to the case at hand. Shannon's results~\cite{shannon1951prediction} on the entropy rate of English suggested an entropy rate of approximately 1 bit per letter, a benchmark that has guided subsequent research in information theory and linguistics [REF]. By our hypothesis,  AI-generated text, due to its reliance on learned patterns, would exhibit an entropy rate  significantly lower than 1.0. This difference in entropy rate, which we demonstrate to be  computable from realistic long-form texts (See \Methods), can serve as a critical marker for distinguishing between the two.


% The importance of our insight is further established from the fact that we can use our estimate of the entropy rate to recognize AI-content without any training. Using the etimated rate to compute the AUC leads to a auc of $> 87\%$. 

%  often reflecting a higher degree of variability and unpredictability than AI-generated text

% While large scale generative AI models (especially LLMs - large language models) have had profound impact across scientific domains in recent years, a variety of concerns about the risks associated to such methods have been raised. Primary concerns relate to both the trustworthiness of model outputs and ethical concerns related to the potential for abuse by bad actors\cite{thirunavukarasu2023large, crothers2023machine}.  Such concerns have underscored the necessity for tools which can reliably distinguish content which has been generated by an AI model vs. content which is human-generated. 

% Conjecturing that AI-generated text is typically less structurally rich than human-generated text, we hypothesize that AI-generated text may often be detectable by estimating the complexity of the text-generation process. Crucial to this approach is a novel, efficient method for estimating the entropy rate of this process. ~\cite{thirunavukarasu2023large, crothers2023machine}


#TODO

1.Abstract over-claims vs. what is actually shown
The abstract rhetorically merges the training-free entropy-rate result (AUC ≈ 0.82) with the calibrated GP result (AUC ≈ 0.99). This will almost certainly be flagged. The fix is to explicitly bifurcate claims: (i) competitive, training-free detection using entropy rate alone; (ii) near-perfect accuracy with a lightweight, calibrated readout. This is mostly a wording correction but has high reviewer impact.

2. “Training-free” vs. “near-perfect” needs a sharp semantic separation
Even outside the abstract, Results and Discussion blur this distinction. Reviewers will insist that “training-free” cannot coexist with a trained GP classifier unless carefully scoped. We should consistently reserve “training-free” for raw entropy-rate thresholds and clearly label the GP as an optional calibration layer.

3. Theoretical framing over-asserts necessity instead of interpretation
Proposition 1 currently reads as if lower entropy rate must follow from algorithmic-statistics assumptions. In reality, the empirical result is doing the work. The theory should be repositioned as interpretive scaffolding (“motivates entropy rate as a meaningful statistic”), not as a premise that forces the outcome. This is a conceptual clarity issue, not a math error.

4. Ambiguity around the shared output-space assumption (|S₁| ≈ |S₂|)
The assumption that human and LLM text draw from comparable output spaces is idealized and will be attacked. We need to explicitly state this is an abstraction induced by the preprocessing and fixed alphabet, not a literal claim about generative support. One clarifying sentence will neutralize a likely reviewer objection.

5. Stationarity assumption is too implicit for document-scale language
Entropy-rate estimation assumes stationarity, but long documents violate this. Right now this is unstated. We should explicitly call it an approximation and, ideally, add a robustness argument (e.g., chunk-wise estimation or sliding windows). Without this, an information-theory reviewer has an easy critique.

6. Independence assumption in Theorem 1 is not defensible as written
The Hoeffding bound assumes independence across thresholds, but all estimates are derived from the same string. This is a real technical vulnerability. Options: weaken the theorem to a heuristic robustness statement, add an explicit independence condition (e.g., disjoint chunks), or reframe as empirical concentration rather than a formal bound.

7. Generator drift claim is imprecisely stated
Figure 1c shows entropy rates increasing over model generations, i.e., the margin is shrinking. Calling this “resilient to generator drift” over-promises. What is true is measurement without retraining, not invariant performance. This needs a careful rephrase to avoid logical inconsistency.

8. Baseline definitions are under-specified (perplexity, ZeroGPT)
Perplexity depends critically on the reference model, tokenization, and whether preprocessing matches your pipeline. ZeroGPT depends on product version and evaluation mode. These must be spelled out, or the comparison will be discounted as non-reproducible.

9. Calibrated GP classifier is insufficiently described
The paper reports near-perfect AUC but underspecifies the feature vector (M, threshold selection), kernel choice, normalization, and hyperparameter handling. This invites skepticism. A short but explicit description is necessary to legitimize the result.

10. Preprocessing choices need justification and at least one sensitivity check
Collapsing text to a 27-symbol alphabet is central to the method, but currently justified only implicitly. Reviewers may ask whether the effect is an artifact of normalization. A brief rationale plus one ablation (e.g., include punctuation) would close this gap.

11. arXiv → pandoc conversion may introduce systematic artifacts
The Methods do not describe how equations, citations, and LaTeX commands are handled. This is not fatal, but a short clarification would prevent suspicion that entropy differences come from conversion noise.

12. Discussion includes speculative claims not needed for acceptance
Claims about the future scarcity of human-authored text are interesting but speculative and uncited. They add little to the core contribution and increase reviewer surface area. Consider softening or removing.

13. Internal logical consistency between figures and narrative
Several strong claims (robustness, drift resistance, universality) are stated in prose but not explicitly tied to a figure or table. Tightening these linkages will make the paper feel more disciplined.

14. Minor language and typographical issues (non-scientific but visible)
Typos, malformed email address, and a few awkward phrasings reduce polish. These are easy fixes but worth cleaning early to avoid distracting reviewers.
