
\section*{Supplementary Methods}



\subsection*{Proof of Proposition 1}
\begin{proof}
For typical \(x \sim G_i\)~\cite{GacsTrompVitanyi2001,LiVitanyi2008},
\cgather{
K(x) = K(S_i) + \log |S_i| + O(1).
}
Taking expectations over \(x \sim G_i\), we have
\cgather{
\mathbb{E}_{x \sim G_i}[K(x)] = K(S_i) + \log |S_i| + O(1).
}
Subtracting, we find
\mltlne{
\mathbb{E}_{x \sim G_1}[K(x)] - \mathbb{E}_{x \sim G_2}[K(x)]\\
= K(S_1) - K(S_2) + \log |S_1| - \log |S_2| + O(1).
}
Invoking our assumption (Eq.~\eqref{eq71}),  this simplifies to
\cgathers{
\mathbb{E}_{x \sim G_1}[K(x)] - \mathbb{E}_{x \sim G_2}[K(x)]
= K(S_1) - K(S_2) + O(1).
}This completes the first part.
Noting that  expected Kolmogorov complexity of strings from  a stationary source satisfies~\cite{horibe03,LiVitanyi2008}:
\cgather{
\lim_{n \to \infty} \frac{1}{n} \mathbb{E}_{x \sim G_i}[K(x)] = H(G_i).
}  establishes the second part.
\end{proof}

\paragraph{Nonparametric entropy-rate estimation. }
We estimate the entropy rate of a stationary ergodic symbol stream using the
PFSA-based nonparametric estimator introduced by Chattopadhyay \emph{et al.}~\cite{CLx,CL12g}.
Given a sequence $s=s_1\cdots s_n$ over a finite alphabet $\Sigma$, the method
constructs empirical next-symbol distributions conditioned on observed substrings
and identifies states of an a priori unknown underlying PFSA generator as equivalence classes of histories that lead to similar future distributions. Rare substrings are excluded via a frequency threshold $m$, ensuring
statistical reliability without assuming a parametric model or access to the
underlying generator. The  entropy rate is then estimated as a weighted average of the Shannon entropies of
empirical symbol-generation distributions associated with that context, with the weights inferred as observed frequencies of the classes of histories that lead to individual states. The estimator is provably consistent under stationarity and
ergodicity, requires no training or reference model, and operates directly on
the observed text stream. Full algorithmic details and theoretical guarantees
are provided in~\cite{CLx,CL12g}.

\paragraph{Pseudocode and Runtime Complexity. } To enhance robustness and mitigate sensitivity to internal thresholds, we apply this estimator \( M \) times across a range of substring frequency thresholds, and report the median of the resulting entropy estimates,  improving concentration guarantees (Theorem~\ref{thmmedian}), with the error probability decaying exponentially in \( M \).

Runtime complexity, including threshold-based pruning and median aggregation over \( M \) thresholds, is \( O(n M |\Sigma|) \), where \( n \) is the length of the observed stream and \( |\Sigma| \)  the alphabet size~\cite{CLx}. Thus, for a fixed $M$, the estimate has input-linear runtime complexity.%% This follows directly from the algorithmic structure of Chattopadhyay \& Lipson (2014), where entropy estimation for each threshold operates in linear time with respect to \( n \), and the median computation requires \( O(M) \) time.

\begin{algorithm}[H]
\caption{Robust Entropy-Rate Estimation}
\KwIn{Symbol stream $x_1^n$, alphabet $\Sigma$, threshold range $\{ m_1, m_2, \ldots, m_M \}$}
\KwOut{Robust entropy-rate estimate $\hat{H}_n$}

\ForEach{$m \in \{ m_1, m_2, \ldots, m_M \}$}{
  Compute $\hat{H}^{(m)}_n$ with Chattopadhyay~\cite{CLx} with threshold $m$ (ignore  substrings with $<m$ occurrences)\;
  }
Compute $
\hat{H}_n \leftarrow \mathrm{median} \left( \hat{H}^{(m_1)}_n, \hat{H}^{(m_2)}_n, \ldots, \hat{H}^{(m_M)}_n \right)
$\;

\Return $\hat{H}_n$\;

\end{algorithm}

% --- Minimal patch: keep your original theorem/proof structure,
% --- but add an "effective independence" qualifier and replace M by M_eff.

\begin{thm}[Robustness and Concentration of the Aggregated Entropy Estimate]\label{thmmedian}
Let \( x_1^N \) be a finite realization from a stationary, ergodic stochastic process over a finite alphabet \( \Sigma \).
Let \( \hat{H}^{(m)}_n \) denote the entropy-rate estimate obtained by applying the algorithm of Chattopadhyay \emph{et al.}~\cite{CLx,CL12g}
with threshold \( m \) (substrings occurring \(<m\) times ignored). Define the aggregated estimator
\[
\hat{H}_n := \mathrm{median}\left\{ \hat{H}^{(m_1)}_n, \hat{H}^{(m_2)}_n, \ldots, \hat{H}^{(m_M)}_n \right\}.
\]
If the indicators \(Z_i\) defined below are independent, then
\[
\mathbb{P}\!\left( \left| \hat{H}_n - H_\star \right| > \epsilon \right)
\leq \exp\!\left( - 2 M \left( \frac{1}{2} - \delta \right)^2 \right).
\]
More generally, when the \(\hat H^{(m_i)}_n\) are obtained from randomly drawn length-\(L\) substreams of a fixed length-\(N\) text and hence may be dependent due to overlap, the same argument applies with an effective number of approximately independent draws
\[
M_{\mathrm{eff}} \;\asymp\; \frac{N}{L},
\]
so that
\[
\mathbb{P}\!\left( \left| \hat{H}_n - H_\star \right| > \epsilon \right)
\;\lesssim\; \exp\!\left( - 2 M_{\mathrm{eff}} \left( \frac{1}{2} - \delta \right)^2 \right).
\]
\end{thm}

\begin{proof}
It follows from Chattopadhyay \emph{et al.}~\cite{CLx,CL12g} that for sufficiently large $n$, for each threshold \( m \in \{ m_1, m_2, \ldots, m_M \} \),
\[
\mathbb{P} \left( \left| \hat{H}^{(m)}_n - H_\star \right| > \epsilon \right) \leq \delta,
\]
for some fixed \( \epsilon > 0 \) and \( \delta < \frac{1}{2} \).
For each \( m_i \), define
\[
Z_i = \mathbf{1} \left\{ \left| \hat{H}^{(m_i)}_n - H_\star \right| > \epsilon \right\},
\qquad \mathbb{E}[Z_i]\le \delta.
\]
The median estimator \( \hat{H}_n \) deviates by more than \( \epsilon \) only if at least half of the individual estimates do:
\[
S = \sum_{i=1}^M Z_i \geq \frac{M}{2}.
\]
If \(Z_1,\ldots,Z_M\) are independent, Hoeffding’s inequality yields
\[
\mathbb{P} \left( S \geq \frac{M}{2} \right) \leq \exp \left( - 2 M \left( \frac{1}{2} - \delta \right)^2 \right),
\]
which implies the stated bound.

If instead the estimates are computed from randomly drawn length-\(L\) substreams of a fixed length-\(N\) text, the resulting \(Z_i\) may be dependent due to overlap. In that case, the same calculation can be interpreted in terms of an effective number of approximately independent draws \(M_{\mathrm{eff}}\asymp N/L\) (i.e., replacing \(M\) by \(M_{\mathrm{eff}}\)), yielding the stated overlap-limited bound.
\end{proof}

\paragraph{Classifier design (trained-\NERO). }
For each document we form a fixed-dimensional feature vector $x\in\mathbb{R}^M$ from the PFSA entropy-rate estimator by evaluating the estimate at a prescribed set of substring-frequency cutoffs $\{m_1,\dots,m_M\}$. Specifically, the $j$th feature is the entropy-rate estimate computed using cutoff $m_j$, so that $x_j=\widehat{H}(m_j)$. Missing feature values (arising when insufficient support is available at a given cutoff) are imputed with zeros using a constant-value imputer.

We evaluate several standard classifiers on these feature vectors, including random forests, extremely randomized trees, AdaBoost, gradient boosting, support vector machines (SVM), and Gaussian-process (GP) classification. Unless otherwise noted, models are fit on a random $50/50$ train-test split of the pooled corpus, and performance is quantified by ROC/AUC on the held-out split. For tree-based models we use class-balanced weighting and $1000-5000$ component estimators to stabilize performance; for SVM we use probabilistic calibration to obtain class probabilities. 

We use GP classification as the trained-\NERO readout as it yields the best performance under cross-validation. The GP classifier is implemented with an RBF kernel,
$k(x,x')=\sigma^2\exp\!\left(-\|x-x'\|^2/(2\ell^2)\right)$, using the standard
\texttt{GaussianProcessClassifier} implementation~\cite{williams2006gaussian}.


\paragraph{Birthtime trajectory fitting (Fig.~1c). }
To summarize how estimated entropy rate of generated text changes across successive LLM models, we fit a
parametric growth curve to the model-specific time series shown in Fig.~\ref{figperf}c. Each model is
represented by a timestamp (``birthtime'') corresponding to its release date, the mean \NERO estimate over generated tests, and an estimated standard
deviation (error bar). Birthtimes were converted to elapsed time in days relative to the
earliest cohort, $x_i=(t_i-t_0)$, yielding observations $(x_i,y_i,\sigma_i)$.

We fit a Richards (generalized logistic) curve with a fixed asymptotic upper bound $U$
(corresponding to mean \NERO estimate for human prose), parameterized as
\[
f(x;L_0,k,x_0,\nu)\;=\;
L_0+\frac{U-L_0}{\left(1+\exp\{-k(x-x_0)\}\right)^{1/\nu}},
\]
where $L_0$ is the lower asymptote, $k$ the growth rate, $x_0$ the inflection location, and
$\nu$ the shape parameter.

Parameters $\theta=(L_0,k,x_0,\nu)$ were estimated by weighted nonlinear least squares using
\texttt{scipy.optimize.curve\_fit} with heteroscedastic weights given by the cohort standard
deviations $\sigma_i$. We used
the initialization $(L_0,k,x_0,\nu)=(\max\{0,\min_i y_i-0.03\}, 0.006, \mathrm{median}(x),\,0.6)$
and box constraints
$
L_0\in[0,\min_i y_i),\quad k\in[10^{-6},3],\quad x_0\in[-2000,5000],\quad \nu\in[0.05,5].
$. The fitted parameters and their asymptotic standard errors were obtained from the
returned covariance estimate $\widehat{\mathrm{Cov}}(\hat\theta)$, and standard chi-square
diagnostics were computed from weighted residuals.% GPT-4.0 is treated as an outlier for the best fit.



\paragraph{Data processing. } For each corpus (human-authored and AI-generated), a structured CSV file was constructed at the document level. Each row corresponds to a single novel and contains the following fields: the entropy-rate estimate computed by NERO, the perplexity-based detector score reported by HowGPT, and the estimated percentage of AI-generated text reported by ZeroGPT. These tables form the basis for all quantitative comparisons reported in the main text and figures.

\paragraph{Topics and text generation. } Topics used to prompt the AI models were obtained from a publicly available repository of narrative prompts (\url{https://www.plot-generator.org.uk/story-ideas/}). The precise semantic content of individual prompts is not critical for long-form text generation; rather, it is sufficient that prompts span a broad range of genres to avoid systematic topical bias. All topics were stored in a JSON file and loaded programmatically during text generation.

Each AI model was prompted with the identical ordered list of topics. This design ensures that differences in estimated entropy rate across models are not attributable to differences in prompt content.

\subsection*{AI text generation prompts}

All AI-generated novels were produced using a fixed prompting protocol designed to elicit long-form narrative prose while minimizing structural or stylistic constraints beyond those necessary for length control and coherence. Identical prompt templates were used across models. Text generation was performed either via official REST APIs or, in some cases, via the OpenAI web interface using the same prompts and default generation settings; no prompt content differed between access modes.

\paragraph{Initial generation prompt.}
Each novel was initiated using the following base prompt, with bracketed fields instantiated programmatically:

\begin{quote}
\footnotesize
I want you to act as a novelist writing about \{topic\}. The total length of the novel is about \{novel\_desired\_length\} characters. After generating each section of the novel, I will tell you how much text you have generated so far. In the novel, include plot development, characters, dialogue, and an overall narrative arc. Generate only the text of the novel itself, without annotations, explanations, or commentary. Write in full paragraphs and chapters of approximately \{chapter\_length\} characters. Continue generating Chapter~1 until I tell you to stop. Generate exclusively novel text, with no metadata, confirmations, or chapter labels.
\end{quote}

\paragraph{Continuation prompt.}
To extend generation beyond the initial context window and reach the target length, generation proceeded iteratively using a continuation prompt. At each step, previously generated text (truncated as necessary) was prepended, followed by the continuation instruction:

\begin{quote}
\footnotesize
\{previously generated text\}

Here is the text generated so far. You have generated approximately \{novel\_current\_length\} characters. Continue writing Chapter~\{current\_chapter\} of the novel. Maintain narrative continuity and write in complete paragraphs. Do not generate annotations, explanations, summaries, or metadata. Generate exclusively novel text.
\end{quote}

\paragraph{AI text generation implementation details. } AI-generated novels were produced using an automated, iterative prompting pipeline implemented in Python. Each novel was generated toward a target length of 150{,}000 characters using repeated completion calls under default model parameters; no temperature, nucleus sampling, or penalty parameters were explicitly set. Only the maximum completion length was controlled.

Chapter indices were inferred implicitly as $\lfloor L / \ell \rfloor + 1$, where $L$ is the cumulative character length generated so far and $\ell=15{,}000$ is the nominal chapter length. Chapters were not explicitly labeled in the generated text.

To support long-form generation within finite model context windows, a sliding-window strategy was employed. When the concatenated prompt exceeded the available context length, only the most recent suffix of the previously generated text was retained, ensuring that the prompt plus maximum completion length remained within the model’s context window.

Generation continued until the target length was reached or until the model declined to continue. Responses yielding fewer than 200 output tokens were treated as refusals; in such cases, all text associated with that generation attempt was discarded and excluded from analysis.

API usage was subject to rate limits on requests per minute and token throughput. When limits were exceeded, generation was paused briefly before resuming. For each novel, all raw API responses and the cumulative generated text were stored in per-document metadata files to enable reproducibility and auditability.
The  generation pipelines are implemented in the following notebooks:
{\footnotesize
\begin{enumerate}
\item gemini:
\url{https://github.com/zeroknowledgediscovery/nero/api_data_collection/gemini_2.5_pro/get_novels.ipynb}
\item gpt4o:
\url{https://github.com/zeroknowledgediscovery/nero/api_data_collection/openai/gpt4o/get_novels.ipynb}
\item gpt5:
\url{https://github.com/zeroknowledgediscovery/nero/api_data_collection/openai/gpt5/get_novels.ipynb}
\item claude:
\url{https://github.com/zeroknowledgediscovery/nero/api_data_collection/claude_sonnet_4/get_novels.ipynb}
\end{enumerate}

\paragraph{LLM access. } AI-generated novels were produced using REST API access or web-interface access to the following models and versions:

\begin{itemize}
\item \textbf{Gemini}: \texttt{gemini-2.5-pro}, accessed between October~8,~2025 and October~16,~2025.
\item \textbf{Claude}: \texttt{claude-sonnet-4-20250514}, accessed between August~26,~2025 and September~22,~2025.
\item \textbf{GPT-4o}: \texttt{gpt-4o-2024-08-06}, accessed between November~13,~2025 and November~14,~2025.
\item \textbf{GPT-5}: \texttt{gpt-5-2025-08-07}, accessed between October~26,~2025 and November~8,~2025.
\end{itemize}


\paragraph{API versus web-interface access. } To mitigate potential bias arising from access modality, AI-generated texts were obtained using a mix of official REST API access and provider web interfaces, as summarized in Table~\ref{tab1}. This design ensures that the reported entropy-rate differences are not artifacts of a particular deployment channel, prompting wrapper, or hidden system configuration. In particular, GPT-4.0 texts were generated using both API-based access and the OpenAI web interface under identical prompting protocols. We observed no statistically significant difference in estimated entropy rates between GPT-4.0 outputs obtained via API versus web-interface access


\paragraph{Human-authored text corpus. } Human-authored novels were retrieved from the Project Gutenberg corpus
(\url{https://gutenberg.org/cache/epub/feeds/}), which provides full-text documents together with structured metadata. All available text and metadata files were downloaded, and only English-language works were retained based on metadata fields. Legal headers and boilerplate text were removed using the \texttt{gutenbergpy} library. Novels shorter than 150{,}000 characters after preprocessing were excluded to ensure sufficient length for stable entropy-rate estimation. 

Technical manuscripts were collected from arXiv via the official OAI-PMH interface and converted from \texttt{.tex} source to plain text. These texts were used as a distinct human-authored reference corpus.


\paragraph{Baseline detectors.}
The HowGPT  (\url{https://howkgpt.nyuad.nyu.edu/}) and the ZeroGPT detectors (\url{https://www.zerogpt.com/}) were accessed between November~2,~2025 and November~25,~2025, with access dates varying by model cohort. From these tools, the reported perplexity-based detector score (HowGPT) and the estimated percentage of AI-generated text (ZeroGPT) were recorded.

For baseline detector comparisons, the central 15{,}000 characters of each novel were extracted and submitted to two deployed detection tools: HowGPT and ZeroGPT. The central segment was used to avoid residual legal headers in human-authored texts; the same procedure was applied to AI-generated texts to maintain parity.
HowGPT/ZeroGPT were queried on fixed-length segments due to interface constraints. \NERO was evaluated on full texts to reflect the intended long-form detection regime.

\paragraph{Perplexity baseline (HowGPT).}
The perplexity-based baseline was implemented using the HowGPT framework, which computes token-level likelihood statistics under a reference language model and aggregates them into a scalar detector score. We use the reported detector score directly rather than raw perplexity values.

\paragraph{ZeroGPT baseline.}
The ZeroGPT baseline was obtained using the deployed ZeroGPT web interface as described above, which returns an estimated fraction of the submitted text labeled as AI-generated (reported as a percentage). We record the percentage score exactly as returned by the interface for each submitted segment, without additional calibration or post-processing. This returned score was used to construct the ROC curve. Because ZeroGPT is a proprietary system whose underlying model, decision rule, and versioning may change over time, the score should be interpreted as an external detector output at the time of access rather than as a reproducible intrinsic statistic of the text.


\paragraph*{Relation to standard LLM benchmarks.}
Existing LLM benchmark suites primarily evaluate task-specific performance (e.g., reasoning accuracy, problem solving, or code synthesis) under controlled prompts and scoring rules. In contrast, NERO estimates a distributional property of long-form generated text—entropy rate under a fixed symbolization—which reflects output complexity rather than task competence. These quantities are not directly commensurate, and strong correspondence is neither assumed nor required: models may score highly on benchmarks while exhibiting low entropy-rate regularity in extended prose. Accordingly, we do not attempt exhaustive benchmark comparisons here. Studying how entropy-rate trajectories relate to aggregate benchmark indicators across model releases is an interesting direction for future work.
