\subsubsection*{$\epsilon$-synchronization}
Next we introduce the notion of $\epsilon$-synchronization of probabilistic automata, which is the basis of our entropy estimation algorithm. In general, synchronization of automata refers to fixing or determining the current state (see Figure~\ref{figsync}); thus it is analogous to contexts in Rissanen's ``context algorithm''~\cite{rissanen83}. While not all PFSAs are synchronizable, it can be shown that all are $\epsilon$-synchronizable in the following sense:
% #######################################
% #######################################
\begin{figure}[t]
%\tikzexternaldisable
\centering 
\input{Figures/figSYNC.tex}
\vspace{-10pt}

\captionN{{\bf Synchronizable and non-synchronizable machines.} Identifying contexts is a key step in estimating the entropy rate of stochastic signals sources; and for PFSA generators, this translates to a state-synchronization problem. However, not all PFSAs are synchronizable, $e.g.$, while the top machine is synchronizable, the bottom one is not. Note that  a history of just one symbol suffices to determine the current state in the synchronizable machine (top), while no finite history can do the same in the non-synchronizable machine (bottom). However, we show that a $\epsilon$-synchronizable string always exists (Theorem~\ref{thmepssynchro}).
}\label{figsync} 
\vspace{-15pt}

\end{figure}
% #######################################
% #######################################

\begin{defn}[$\epsilon$-synchronizing Strings]\label{defepsilonsynchro}
  A  string $x\in \Sigma^\star$ is $\epsilon$-synchronizing for a PFSA if
\cgather{
\exists\bvec  \in \mathcal{E}, \vert \vert \wp_x -\bvec  \vert \vert_\infty \leqq \epsilon.
}
%The norm used is unimportant.
\end{defn}

\begin{thm}[$\epsilon$-Synchronization of Probabilistic Automata]\label{thmepssynchro}
 For any QSP $\mathcal{H}$ over  $\Sigma$ and any $\epsilon > 0$, there exists an $\epsilon$-synchronizing string for the PFSA  $\mathcal{P}_\mathcal{H}$. 
%\cgather{
%\forall \, \epsilon' > 0, \exists x \in \Sigma^\star, \bvec  \in \mathcal{E},  \vert \vert \wp_x -\bvec \vert \vert_\infty \leqq \epsilon'\label{eqsync}
%}
%where the norm used is unimportant.
\end{thm}
\begin{proof}
See \cite{CLx}.
\end{proof}
%Theorem~\ref{thmepssynchro} induces the notion of  $\epsilon$-synchronizing strings, and guarantees their existence for arbitrary PFSA.
%\vspace{5pt}

While Theorem~\ref{thmepssynchro} guarantees the existence of  $\epsilon$-synchronizing strings for an arbitrary PFSA, it does not give an algorithm for computing them (see Theorem~\ref{thmderivheap}). However, the construction used implies that at most $O(1/\epsilon)$ strings need to be analyzed to find an $\epsilon$-synchronizing string\cite{CLx}.

%
%We next introduce the notion of symbolic derivatives:
\subsubsection*{Symbolic Derivatives}
Computation of $\epsilon$-synchronizing strings requires the notion of symbolic derivatives. Note that, PFSA states  are not  observable; we observe  symbols generated from hidden states. A symbolic derivative at a given string  specifies the distribution of the next symbol over the alphabet. Below, we denote the set of probability distributions over a finite  set of cardinality $k$ as $\mathscr{D}(k)$.

%First, we specify a count function.
\begin{defn}[Symbolic Count Function]\label{defcount}
 For a string $s$ over  $\Sigma$, the count function $\#^s: \Sigma^\star \rightarrow \mathbb{N}\cup \{0\}$,  counts the number of times a particular substring occurs in $s$. The count is overlapping, $i.e.$, in a string $s=0001$, we count the number of occurrences of $00$s as $\underline{00}01$ and $0\underline{00}1$, implying $\#^s 00 =2$.
\end{defn}

\begin{defn}[Symbolic Derivative]\label{defsymderivative}
 For a string $s$  generated by a QSP over $\Sigma$, the symbolic derivative  $\phi^s:\Sigma^\star \rightarrow \mathscr{D}(\vert \Sigma\vert -1)$ is defined:
\vspace{-5pt}
\cgather{
\phi^s(x) \big \vert_i = \frac{\#^s x\sigma_i}{\sum_{\sigma_i \in \Sigma }\#^s x\sigma_i}
}
Thus,  $\forall \, x \in \Sigma^\star, \phi^s(x)$ is a probability distribution over $\Sigma$. $\phi^s(x)$ is referred to as the symbolic derivative at $x$.
\end{defn}

Note that  $\forall \, q_i \in Q$, $\pitilde$  induces a  probability distribution over $\Sigma$ as  $[\pitilde(q_i,\sigma_1), \cdots , \pitilde(q_i,\sigma_{\vert \Sigma \vert})]$. We denote this as $\pitilde(q_i,\cdot)$. We next show that the symbolic derivative at $x$ can be used to estimate this distribution for $q_i = [x]$, provided $x$ is $\epsilon$-synchronizing.

\begin{thm}[$\epsilon$-Convergence]\label{thmsymderiv} If $x \in \Sigma^\star$ is $\epsilon$-synchronizing, then:
 \cgather{
\forall \, \epsilon > 0,  \lim_{\vert s \vert \rightarrow \infty}\vert \vert \phi^s(x) -\pitilde([x],\cdot)\vert \vert_\infty \leqq_{a.s} \epsilon\label{eqsync2} 
% 
}
\end{thm}
\begin{proof}
See \cite{CLx}.
\end{proof}
\subsubsection*{Computation of  $\epsilon$-synchronizing Strings}
We now describe identification of  $\epsilon$-synchronizing strings given a sufficiently long observed string ($i.e.$ a sample path) $s$. %Theorem~\ref{thmepssynchro}  guarantees existence, and Corollary~\ref{corsynchrodepth} establishes that $O(1/\epsilon)$ substrings need to be analyzed till we encounter an $\epsilon$-synchronizing string.
% 
%  What these results do not tell us, is how we would determine that we have indeed found such a string; since we do not have the PFSA (which is what we are trying to construct), there is no  direct way of checking for $\epsilon$-synchronization. 
The key insight is to inspect the geometric structure of the set of probability vectors over $\Sigma$, obtained by constructing $\phi^s(x)$ for different choices of the candidate string $x$.
% , offers a solution.

% Recall that the set $\mathscr{D}(\vert \Sigma \vert -1)$ denotes the set of probability distributions over $\Sigma$.
% Also, for a set 
\begin{defn}[Derivative Heap]\label{defderivheap}
  Given a string $s$ generated by a QSP, a derivative heap $\mathcal{D}^s: 2^{\Sigma^\star} \rightarrow \mathscr{D}(\vert \Sigma \vert -1)$ is the set of probability distributions over $\Sigma$ calculated for a  subset of strings  $L \subset \Sigma^\star$ as:
\cgather{
\mathcal{D}^s(L) = \big \{ \phi^s(x): x \in L \subset \Sigma^\star\big \}
}
%  
\end{defn}
% #######################################
\begin{lem}[Limiting Geometry]\label{lemlimderiv}
 Let us define:
\cgather{
\mathcal{D}_\infty = \lim_{\vert s \vert \rightarrow \infty }\lim_{L \rightarrow \Sigma^\star} \mathcal{D}^s(L)
}
If $\mathscr{U}_\infty$ is the convex hull of $\mathcal{D}_\infty$, and $u$ is a vertex of $\mathscr{U}_\infty$, then 
\cgather{
\exists q \in Q, \textrm{such that } u=\pitilde(q,\cdot)
}
\end{lem}

\begin{proof}
Recalling Theorem~\ref{thmsymderiv}, the result follows from noting that any element of $\mathcal{D}_\infty$ is a convex combination of elements from the set $\{\pitilde(q_1,\cdot), \cdots , \pitilde(q_{\vert Q\vert},\cdot)  \}$.
\end{proof}

Lemma~\ref{lemlimderiv} does not claim that the number of vertices of the convex hull of $\mathds{D}_\infty$ equals the number of states, but that every vertex  corresponds to a state.  We cannot generate $\mathcal{D}_\infty$  since we have a finite observed string $s$, and we can  calculate $\phi^s(x)$ for a finite number of  $x$. Instead, we show that  choosing a string corresponding to the vertex of the convex hull of the  heap, constructed by considering $O(1/\epsilon)$ strings, gives us an $\epsilon$-synchronizing string with high probability.

\begin{thm}[Derivative Heap Approx.]\label{thmderivheap}
 For  $s$ generated by a QSP, let $\mathcal{D}^s(L)$ be computed with $L=\Sigma^{O(log(1/\epsilon))}$. If for  $x_0 \in \Sigma^{O(log(1/\epsilon))}$,  $\phi^s(x_0)$  is a vertex of the convex hull of $\mathcal{D}^s(L)$, then 
\cgather{
\operatorname{Pr}(\textrm{$x_0$ is not $\epsilon$-synchronizing}) \leqq e^{-\vert s \vert \epsilon p_0}
}
 where $p_0$ is the probability of encountering  $x_0$ in $s$.
\end{thm}

% \begin{proof}
%  The result follows from Sanov's Theorem~\cite{Cs84} for convex set of probability distributions. See Appendix for details.
% \end{proof}
% 
\begin{proof}
See \cite{CLx}.
\end{proof}
%%%%%


However, if the underlying PFSA model is not available, and we have only a symbolic stream generated by a QSP, then  Eq.~\eqref{eqnEPFSA} cannot be directly employed to estimate the entropy rate. In that case, one possibility is to first infer the hidden PFSA using the algorithm reported in \cite{CL12g}, and then estimate the entropy rate from Eq.~\eqref{eqnEPFSA}. However, if we are only interested in the latter, then we do not need to infer the complete generative model and there exists a more parsimonious approach to estimate the entropy rate directly.  First, we need a lemma which bounds the deviation in entropy for deviations in the probability distribution in the discrete case.
%
%#########################################################
\input{Figures/algo.tex}
%#########################################################
%#########################################################
%% 
%

\begin{lem}[Bound on Entropy Deviation]\label{lementropydev}
For probability distributions $p,q$ on a finite set $\Sigma$ and all $\epsilon \in (0,1)$, if $\vert \vert p -q \vert \vert_\infty \leqq \epsilon$, the entropies $H(p),H(q)$ satisfy: \[
\vert H(p) - H(q) \vert <  \epsilon' \log \frac{\vert \Sigma \vert-1}{\epsilon'} + (1-\epsilon') \log \frac{1}{1-\epsilon'} \notag
,\]
where $\epsilon' = \left \{ \begin{array}{cl}
\epsilon & \textrm{if } \epsilon \leqq 1 / 2 \\
1 - \epsilon & \textrm{otherwise.} 
\end{array}\right. \notag$
\end{lem}

\begin{proof}
See \cite{CLx}.
\end{proof}
We denote this bound on entropy deviation as the generalized binary entropy function $\mathds{B}(\epsilon, \vert \Sigma \vert )$:
\begin{defn}[Generalized Binary Entropy Function]\label{defbef}
\cgather{
\mathds{B}(\epsilon, \vert \Sigma \vert ) = \epsilon' \log \frac{\vert \Sigma \vert-1}{\epsilon'} + (1-\epsilon') \log \frac{1}{1-\epsilon'}\\
\textrm{where } \epsilon' = \left \{ \begin{array}{cl}
\epsilon & \textrm{if } \epsilon \leqq 1 / 2 \\
1 - \epsilon & \textrm{otherwise.} 
\end{array}\right. \notag
}
\end{defn}

%As a corollary to Lemma~\ref{lementropydev}, we have the following result:
\begin{cor}[To Lemma~\ref{lementropydev}]\label{firstapprox}
Given a symbol stream generated by a PFSA  $G=(Q,\Sigma,\delta,\pitilde)$, and an $\epsilon$-synchronizing string $x_0$, we have:
\cgathers{
\left \vert \lim_{n\rightarrow \infty} \frac{1}{\vert \Sigma^n_+ \vert} \sum_{x \in \Sigma^n_+} \lim_{\vert s \vert \rightarrow \infty}H(\phi^s(x_0 x)) - H(G) \right \vert < \mathds{B}(\epsilon, \vert \Sigma \vert )
}
\end{cor}
%%
\begin{proof}
See \cite{CLx}.
\end{proof}
%
Corollary~\ref{firstapprox} suggests an approach to estimating the entropy rate of a PFSA within a specified precision: for large $n$, compute the entropy rate of the symbolic derivative at all strings of length at most $n$, prefixed by an $\epsilon$-synchronizing string. In practice, however, it is in fact possible (and more useful) to derive a similar estimate in terms of an approximate (in the sense of Theorem~\ref{thmderivheap}) $\epsilon$-synchronizing string and a finite number of independently chosen sample points\cite{CLx}.  Thus, this is the approach used in Algorithm~\ref{algo1}. 

%Our main result is that the entropy rate of the PFSA can be approximated by computing the derivative at an arbitrary set of strings in $\Sigma^\star$:
%\begin{thm}[Approximation Theorem]\label{thmB2}
%Given a finite string $s$ generated by a PFSA  $G=(Q,\Sigma,\delta,\pitilde)$, and a string  $x_0 \in \Sigma^\star$ satisfying the pre-conditions described in Theorem~\ref{thmderivheap}, 
%and an $\epsilon$-synchronizing string $x_0$, 
%we have for  any independently chosen set of strings $\ND \subseteqq \Sigma^\star$:
%\[
%Pr \Bigg ( \bigg \vert   \frac{1}{\vert \ND \vert} \sum_{\mathclap{x \in \ND}} H(\phi^s(x_0 x))   - H(G) \bigg \vert > \epsilon +2\mathds{B}(\epsilon, \vert \Sigma \vert ) \Bigg ) \notag  \leqq C_0 \frac{1+\epsilon^2}{\vert s \vert \epsilon^{3}} + 2 e^{-C_1\vert \ND \vert  \epsilon^{2} } + e^{-\epsilon p_0 \vert s \vert} \label{eqBND}
%\]
%where $C_0=(8/e+8/e^2)(\vert \Sigma \vert -1)$,    $C_1=2 / \log^2 \vert \Sigma \vert$ and $p_0$ is the non-zero occurrence probability of $x_0$ in $s$.
%\end{thm}
%\begin{proof}
%See \cite{CLx}.
%\end{proof}

\subsubsection*{Algorithmic Implementation}\label{secimpl}
Our approach to entropy rate estimation is outlined in Algorithm~\ref{algo1}. Inputs to the algorithm are the data stream $s$, precision $\epsilon > 0$, and confidence level $\alpha$ at which the error estimate is desired. Importantly, the size of the set of sampled string $\ND$ is not required to be an input; if computational effort is not a concern, then the uncertainty contribution from the term involving $\vert \ND \vert$ can be reduced to negligible levels by using a sample set with  
$\vert \ND \vert \simeq \frac{K}{2\epsilon^2} \log^2 \vert \Sigma \vert $, which would result in  uncertainty contribution of $\sim e^{-K}$. Using $\vert \ND \vert \simeq 10^7 \log^2 \vert \Sigma \vert$ is generally sufficient to make this factor negligible; smaller sets may be used under computational constraints, which would  lead to  increased uncertainty in the entropy estimate.  Particularly rare strings may accumulate errors, which is prevented in the implementation by ignoring strings that occur too infrequently (see $N_\mathrm{min}$ in steps 5 and 15 of Algorithm~\ref{algo1}).