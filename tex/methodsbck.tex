%%  \subsection*{Algorithmic Estimation of Entropy Rate of Symbol Streams}

%% Our approach to estimating entropy of the text generation process is based on modeling the process via probabilistic automata. We describe this approach in a general context. % Our automata models are distinct from others reported in the literature~\cite{P71,VTCC05}. 
%% The details of this formalism can be found in \cite{CL12g}; we include a brief overview here for the sake of completeness, outlining the connection of stochastic processes to PFSA generators. 

%% %\subsubsection*{Stochastic Processes \& Entropy Rate}

%% Let $\Sigma$ be a finite alphabet of symbols with size $\abs{\Sigma}$. The set of sequences of length $d$ over $\Sigma$ is denoted by $\Sigma^d$. The set of arbitrary finite length sequences is denoted by $\Sigma^\star$ (the Kleene star operation \cite{hopcroft2008introduction}), \ie $\Sigma^{\star} = \bigcup_{d=0}^{\infty}\Sigma^{d}$. We use lower case Greek, for example $\sigma$ or $\tau$, for symbols in $\Sigma$, and lower case Latin, for example $x$ or $y$, for sequences of symbols, \ie $x=\sigma_1\sigma_2\dots\sigma_n$. $\abs{x}$ is the length of $x$.  $\lambda$ is the  empty sequence. %We denote the set of strictly infinite sequences over $\Sigma$ by $\Sigma^\omega$, and the set of strictly infinite sequences having $x$ as prefix by $x\Sigma^\omega$. Also, $\Sigma^d_+ = \{x \in \Sigma^\star : \vert x \vert  \leqq d\}$.

%% \begin{defn}[Stochastic Process over $\Sigma$]
%% \label{defn:StochasticProcessOverSigma}
%% A stochastic process over $\Sigma$ is a collection of $\Sigma$-valued random variables $\set{X_t}_{t\in\mathbb{N}}$ indexed by positive integers~\cite{doob1990stochastic}.
%% \end{defn}

%% In this work, we only consider strictly stationary and ergodic\cite{peebles2001probability} processes. Given a stochastic process $\mathcal{X} = \{X_t\}$, the entropy rate $H(\mathcal{X})$ of the process is defined as:
%% %
%% %\subsubsection*{Entropy Rate of Stochastic Processes}
%% %The entropy rate of a stochastic process $\mathcal{X}=\{X_i\}$ is defined as:
%% \[\label{eqentropy}
%% H(\mathcal{X}) = \lim_{n \rightarrow \infty} \frac{1}{n} H(X_1,\cdots,X_n),
%% \]
%% %The entropy $H(X)$ of a discrete random variable $X$, taking values in the alphabet $\Sigma$, is defined as:
%% %\[
%% %H(X) = - \sum_{x \in \Sigma} p(x)\log p(x),
%% %\]where $p(x)$ is the probability of occurrence of $x \in \Sigma$. Intuitively, it can be shown that the entropy of a random variable is the average length of its shortest description~\cite{cover}.
%% where $H(X_1,\ldots,X_n)$ denotes the joint entropy of the random variables $X_1,\ldots,X_n$:
%% \[
%% H(X_1,\ldots,X_n) = - \sum_{x_i \in \Sigma_i} p(x_1,\cdots, x_n) \log p(x_1,\cdots, x_n). 
%% \]
%% % The conditional entropy of a random variable is  the expected entropy of the conditional distributions:
%% % \cgather{
%% % H(Y\vert X)  =  \sum_{x \in \Sigma} p(x) H(Y\vert X=x)
%% % }
%% %The chain rule for entropy calculations~\cite{cover} follows from the definitions, and is of particular importance:
%% %\cgather{\label{eqchrule}
%% %H(X_1,\cdots,X_n)= \sum_{i=1}^n H(X_i \vert X_{i-1},\cdots,X_1)
%% %}
%% %The notion of entropy formalizes the Asymptotic Equipartion Property (AEP): If discrete random variables  $X_1,\cdots, X_n$ are i.i.d. and have probability mass function $p(x)$, then we have:
%% %\cgather{
%% %-\frac{1}{n} \log p(X_1,\cdots, X_n) \xrightarrow{a.s} H(X) = - \sum_{x \in \Sigma} p(x)\log p(x)
%% %}
%% %The AEP implies that $nH(X)$ bits suffice on average to describe $n$ i.i.d. random variables. If the random variables are not independent, the entropy $H(X_1,\cdots,X_n)$ still grows asymptotically linearly with $n$ at a rate known as the entropy rate of the process. \
%% %In particular, if the random variables  define a stationary ergodic stochastic process $\mathcal{X} = \{X_i\}$, then the AEP still holds:
%% %\cgather{
%% %-\frac{1}{n}\log p(X_1,\cdots, X_n) \xrightarrow{\textrm{a.s.}} H(\mathcal{X})
%% %}
%% As in the case of i.i.d variables, \textit{typical sequences} of length $n$  may be represented using approximately $nH(\mathcal{X})$ bits. Thus the entropy rate  quantifies the average description length of the process, and hence its expected complexity~\cite{horibe03}. 


%% %Classically, automaton states are equivalence classes for the Nerode relation; two strings are  equivalent if and only if any finite extension of the strings is either both in the language under consideration, or neither are~\cite{HMU01}. We use a probabilistic extension~\cite{CR08}.

%% % Let $\mathcal{F}$ denote the smallest $\sigma$-algebra generated by the family of sets $\SR = \{ x \Sigma^\omega : x \in \Sigma^\star\}$. Then every stochastic process over $\Sigma$ induces a probability space $(\Sigma^\omega,\mathcal{F},\mu)$.\cite{CLx} For notational brevity, we will denote $\mu( x \Sigma^\omega)$ as $\operatorname{Pr}(x)$. More generally, a stochastic process induces a measure for each sequence $x$, which we may then use to define an equivalence relation on $\Sigma^\star$:
%% % %
%% % \begin{defn}[Sequence-Induced Measure $\mu_{x}$]
%% % The measure $\mu_x$ induced by a sequence $x$ is the extension to $\mathcal{F}$ of the premeasure defined on $\SR$ given by
%% % \[
%% % %\label{eq:DefinitionOfPremeasure}
%% %  \forall\, x,y\in\Sigma^{\star},  \ \ \mu_x\paren{y\Sigma^{\omega}} = \frac{\operatorname{Pr}(xy)}{\operatorname{Pr}(x)}, \text{ \ if } \operatorname{Pr}(x) > 0.
%% % \]
%% % \end{defn}
%% % %From the definition above we see that $\mu=\mu_{\lambda}$.
%% % \begin{defn}[Probabilistic Nerode Equivalence \cite{chattopadhyay2008structural}]
%% % \label{defn:NerodeEquiv}
%% % For any pair of sequences $x, y\in\Sigma^{\star}$, $x$ is equivalent to $y$ under the probabilistic Nerode equivalence (written as $x\sim_N y$) if and only if either $Pr(x) = Pr(y) = 0$ or $\mu_x = \mu_y$. The equivalence class of a sequence $x$ is denoted by $[x]$ and is called a causal state~\cite{chattopadhyay2014data}. The cardinality of the set of causal states is called the probabilistic Nerode index.
%% % \end{defn} Note that $\sim_{N}$ is right invariant equivalence: $x \sim_N y \implies xz \sim_N yz$ for all $z \in \Sigma^\star$.
%% %\cgather{
%% %x \sim_{N} y \Rightarrow \forall \, z \in \Sigma^\star, xz \sim_{N} yz.
%% %}
%% %Since a right-invariant equivalence on $\Sigma^\star$ induces an automaton structure, the probabilistic Nerode relation induces a probabilistic automaton where the states are equivalence classes of this relation (In practice, approximate merges may yield coarse-grained versions of these causal states (see SI, Section II-B)). This construction motivates the following definition.

%% % #######################################
%% \begin{defn}[Probabilistic Finite-State Automaton (PFSA)]
%% \label{defn:PFSA}
%% Let $\mathbf{P}_\Sigma$ denote the space of probability distributions over $\Sigma$. A PFSA $G$ is a quadruple $(Q, \Sigma, \delta, \pitilde)$, where:
%% \begin{align*}
%%    & Q \text{ is a finite set (the states),}\\
%%    & \Sigma \text{ is a finite alphabet (the symbols),}\\
%%    &  \delta: Q\times\Sigma \rightarrow \Sigma \text{ is a map (the transition map),}\\
%%    &  \pitilde: Q \rightarrow \mathbf{P}_\Sigma \text{ is a map  (the transition probability map).}
%% \end{align*}Notation: the entry of $\pitilde(q)$ indexed by $\sigma$ is  $\pitilde(q,\sigma)$. 
%% \end{defn}
%% %$Q$ is the set of nodes of the directed graph of the PFSA. We refer to these nodes as ``states''; however \textit{they might not coincide with the set of causal states} defined as equivalence classes of the probabilistic Nerode relation (See Defn.~\ref{defn:NerodeEquiv}). Here, we will assume all PFSAs have a strongly connected graph, and thus have a unique stationary distribution $\wp_{G}$.
%% %Note that the probability of the null word is unity from each state. If the current state and next symbol are  specified, our next state is fixed, similar to Probabilistic Deterministic Automata~\cite{Gavalda06}. However, unlike the latter, we lack final states in the model; additionally, we assume our graphs to be strongly connected. 
%% %
%% A PFSA $G=\paren{Q, \Sigma, \delta, \pitilde}$ acts as a generative model for symbolic sequences. Given an arbitrary distribution $\wp$ on $Q$, we may first sample to obtain a state $q_i$, then use the maps $\pitilde$ and $\delta$ to obtain a symbol $\sigma_i$ and new state $q_{i+1} = \delta(q_1, \sigma_1)$. Conversely, given a PFSA $G$, initial distribution $\wp_0$, and sequence $x$, a distribution $\wp_{G, \wp_0}(x)$ is induced over the states in the natural way, where the entry $\wp_{G, \wp_0}(x, q)$ gives the probability that $G$ has output $x$ and is in state $q$  (for brevity, when $\wp_0=\wp_{G}$ we may write $\wp_{G, \wp_0}(x)$ as $\wp_{G}(x)$, or simply $\wp(x)$). This distribution over states motivates the following definition.

%% \begin{defn}[Stochastic Process Generated by a PFSA]
%% \label{defn:StochasticProcessOfPFSA}
%% Let $G=\paren{Q, \Sigma, \delta, \pitilde}$ be a PFSA and let $\wp_0$ be a distribution on $Q$. Then we say that the stochastic process generated by $G$ and $\wp_0$ is the $\Sigma$-valued process $\set{X_t}_{t\in\Sigma}$ defined by: \begin{align*}
%%     X_1 &\sim \wp_0\\
%%     X_{t+1} &\sim \wp_{G, \wp_0}(X_{1} \cdots X_{t}), \text{ for } t\in\N.
%% \end{align*}
%% \end{defn}


%% \begin{thm}
%% \label{thm:FiniteCausalStatePFSAGen}
%% If $\mathcal{X}$ is a stationary ergodic process with finite number of causal states, then $\mathcal{X}$ is generated by a strongly connected PFSA.  
%% \end{thm}
%% \begin{proof}
%% See Theorem 2 in Supplementary Text, II-C.
%% \end{proof}
%% %It can be further shown that given a QSP $\mathcal{H}$, the generating initial-marked PFSA above induces a canonical representation which is independent of the initial state\cite{CLx}. Thus, we may in fact refer to \emph{the} initial-marked PFSA induced by a QSP, which we denote as $\mathcal{P}_\mathcal{H}$, and refer to it simply as a ``PFSA''.  States in $\mathcal{P}_\mathcal{H}$ are representable as states in $\mathcal{C}_\mathcal{H}$ as elements of $\mathcal{E}$, where
%% %\begin{align*}
%% %    &\mathcal{E} = \{e^i \in [0, 1]^{|Q|} : i = 1,\ldots, |Q|\},\\
%% %    &\text{for } e^i|_{j} = \begin{cases} 1, & \text{if } i=j\\0, & \text{otherwise}.\end{cases}
%% %\end{align*} %Next we show that  we always encounter a state arbitrarily close to some element in  $\mathcal{E}$ in the canonical construction starting from the stationary distribution $\wp_\lambda$ on the states of $\mathcal{P}_\mathcal{H}$.
%% %\vspace{5pt}
%% % 



%% %\subsubsection*{Computing Entropy Rate of PFSA-generated Processes}
%% Importantly, if we are given a PFSA generator for a stochastic process, the entropy rate is computable in closed form:
%% %\begin{thm}[Closed-form Formula for Entropy Rate]
%% %\label{thm:Closed-formFormulaForEntropyRate}
%% %The entropy rate of the stochastic process generated by the PFSA $G = \paren{\Sigma, Q, \delta, \pitilde}$ is given by:
%% %\cgather{\label{eqnEPFSA}
%% %    H(G) = \sum_{q\in{Q}}\wp _{G}(q)\cdot H(\pitilde(q)).
%% %}
%% %\end{thm}
%% %\begin{proof}
%% %Omitted.
%% %\end{proof}
%% \begin{thm}\label{thm:Closed-formFormulaForEntropyRate}
%% The entropy rate $H(G)$ (in bits) for the stochastic process generated by a PFSA $G=(Q,\Sigma,\delta,\pitilde)$ is given by:
%% \cgather{\label{eqnEPFSA}
%% H(G) = \sum_{i=1}^{\vert Q\vert} \wp_G\big \vert_i \sum_{\sigma_j \in \Sigma} \pitilde(q_i,\sigma_j) \log\pitilde(q_i,\sigma_j)
%% .}
%% \end{thm}
%% \begin{proof}
%% Denote the stochastic process generated by $G$ as  $\mathcal{X}= \{X_i\}$. Using the chain rule for entropy\cite{cover2012elements}, we have:
%% \[
%% H(G) = \lim_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n H(X_i \vert X_{i-1},\cdots,X_1)
%% \]
%% Since $G$ is always at some state $q \in Q$, we conclude that for any $i$:
%% \cgathers{
%%  H(X_i \vert X_{i-1},\cdots,X_1) \in \left \{ \sum_{\sigma_j \in \Sigma} \pitilde(q,\sigma_j) \log\pitilde(q,\sigma_j) : q \in Q  \right \}
%% }
%% Furthermore, since $G$ is strongly connected, and therefore has a unique stationary distribution $\wp_G$~\cite{St97}, the number of times state $q_i$ occurs approaches $n\wp_G \big\vert_i$ as $n \rightarrow \infty$.
%% \end{proof}

%% Together, Theorems \ref{thm:FiniteCausalStatePFSAGen} and \ref{thm:Closed-formFormulaForEntropyRate} provide the basis for our estimation of entropy rate of text generation.  If the underlying PFSA model is not available and we have only a symbolic stream, then we may first infer the PFSA (eg. using the algorithm reported in \cite{CL12g}), and then estimate the entropy rate from ~\eqref{eqnEPFSA}.


\subsection*{Pseudocode and Runtime Complexity}
%\usepackage{algorithm}
%\usepackage{algorithmic}
We adopt the entropy estimation algorithm of Chattopadhyay et al.~\cite{CLx} as the core estimator. To enhance robustness and mitigate sensitivity to internal thresholds, we apply this estimator \( M \) times across a range of substring frequency thresholds, and report the median of the resulting entropy estimates. As shown in Theorem~ref{thmmedian}, this aggregation step improves concentration guarantees, with the probability of estimation error decaying exponentially in \( M \).

The total runtime complexity of the entropy-rate estimation procedure, including threshold-based pruning and median aggregation over \( M \) thresholds, is \( O(n M |\Sigma|) \), where \( n \) is the length of the observed symbol stream and \( |\Sigma| \) is the alphabet size~\cite{CLx}. %% This follows directly from the algorithmic structure of Chattopadhyay \& Lipson (2014), where entropy estimation for each threshold operates in linear time with respect to \( n \), and the median computation requires \( O(M) \) time.

\begin{algorithm}[H]
\caption{Robust Entropy-Rate Estimation}
\KwIn{Symbol stream $x_1^n$, alphabet $\Sigma$, threshold range $\{ m_1, m_2, \ldots, m_M \}$}
\KwOut{Robust entropy-rate estimate $\hat{H}_n$}

\ForEach{$m \in \{ m_1, m_2, \ldots, m_M \}$}{
  Compute $\hat{H}^{(m)}_n$ with Chattopadhyay~\cite{CLx} with threshold $m$ (ignore  substrings with $<m$ occurrences)\;
  }
Compute $
\hat{H}_n \leftarrow \mathrm{median} \left( \hat{H}^{(m_1)}_n, \hat{H}^{(m_2)}_n, \ldots, \hat{H}^{(m_M)}_n \right)
$\;

\Return $\hat{H}_n$\;
\end{algorithm}

%% \begin{algorithm}
%% \caption{Learning-Free Entropy Rate Estimation}
%% \begin{algorithmic}
%% \Require Raw text document $T$
%% \Ensure Estimated entropy rate $H(T)$

%% \State \textbf{Preprocessing:}
%% \State Convert all characters in $T$ to lowercase
%% \State Remove punctuation, digits, and newlines
%% \State Replace  consecutive whitespace  with a single space
%% \State Map cleaned text to symbol stream $x \in \Sigma^n$, with $\Sigma = \{\texttt{a-z}, \texttt{space}\}$

%% \State \textbf{Entropy Estimation:}
%% \State Infer PFSA $G = (Q, \Sigma, \delta, \tilde{\pi})$ from $x$ as in~\cite{CL12g}
%% \State Compute stationary distribution $\mathbb{P}_G$
%% \State Compute:
%% \[
%% H(T) = -\sum_{q \in Q} \mathbb{P}_G(q) \sum_{\sigma \in \Sigma} \tilde{\pi}(q, \sigma) \log_2 \tilde{\pi}(q, \sigma)
%% \]

%% \Return $H(T)$
%% \end{algorithmic}
%% \end{algorithm}




%% \begin{thm}[Correctness and Consistency of the NERO Entropy-Rate Estimator]
%% Let $\mathcal{X} = \{X_t\}_{t \ge 1}$ be a stationary and ergodic stochastic process over a finite alphabet $\Sigma$. Let $x_1^n = x_1 x_2 \dots x_n$ be a finite realization from $\mathcal{X}$. Then:

%% \begin{enumerate}
%% \item The true entropy rate of the process is given by:
%% \mltlne{
%% H^\star = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \dots, X_n) \\= \lim_{s \to \infty} \sum_{w \in \Sigma^s} P(w) \left[ -\sum_{\sigma \in \Sigma} P(\sigma \mid w) \log_2 P(\sigma \mid w) \right].
%% }

%% \item The NERO entropy-rate estimator $\widehat{H}_n$, defined below, converges in probability to $H^\star$ as $n \to \infty$.
%% \end{enumerate}
%% \end{thm}

%% \begin{proof}
%% We first derive the entropy-rate formula. From the definition of entropy rate:
%% \cgather{
%% H^\star = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \dots, X_n),
%% }
%% and by stationarity,
%% \cgather{
%% H^\star = \lim_{n \to \infty} H(X_n \mid X_1^{n-1}).
%% }
%% Fix a context length $s \ge 1$. Then
%% \cgather{
%% H(X_n \mid X_1^{n-1}) = H(X_{s+1} \mid X_1^s).
%% }
%% Expanding the conditional entropy gives:
%% \cgather{
%% H(X_{s+1} \mid X_1^s) = -\sum_{w \in \Sigma^s} \sum_{\sigma \in \Sigma} P(w\sigma) \log_2 P(\sigma \mid w),
%% }
%% and substituting $P(w\sigma) = P(w) P(\sigma \mid w)$ yields:
%% \cgathers{
%% H(X_{s+1} \mid X_1^s) = \sum_{w \in \Sigma^s} P(w) \left[ - \sum_{\sigma \in \Sigma} P(\sigma \mid w) \log_2 P(\sigma \mid w) \right].
%% }
%% Taking the limit $s \to \infty$ gives the desired representation of $H^\star$.

%% We now define the NERO estimator and prove convergence. Fix maximum context depth $s_{\max} \ge 1$ and threshold $m \ge 1$. For each context $w \in \Sigma^{\le s_{\max}}$, let:
%% \begin{itemize}
%% \item $N_n(w)$ = count of $w$ in $x_1^n$,
%% \item $N_n(w\sigma)$ = count of $w$ followed by $\sigma$,
%% \item empirical conditional probability:
%% \cgather{
%% \hat{P}_n(\sigma \mid w) := \frac{N_n(w\sigma)}{\sum_{\tau \in \Sigma} N_n(w\tau)},
%% }
%% \item empirical context weight:
%% \cgather{
%% \hat{P}_n(w) := \frac{N_n(w)}{\sum_{v \in \mathcal{T}_m^{(n)}} N_n(v)},
%% }
%% where $\mathcal{T}_m^{(n)} := \{ w \in \Sigma^{\le s_{\max}} : N_n(w) > m \}$.
%% \end{itemize}

%% Then the NERO entropy estimate is:
%% \cgather{
%% \widehat{H}_n^{(m)} := -\sum_{w \in \mathcal{T}_m^{(n)}} \hat{P}_n(w) \sum_{\sigma \in \Sigma} \hat{P}_n(\sigma \mid w) \log_2 \hat{P}_n(\sigma \mid w).
%% }

%% We may optionally average over $m = 1, \dots, M$:
%% \cgather{
%% \widehat{H}_n := \frac{1}{M} \sum_{m=1}^{M} \widehat{H}_n^{(m)}.
%% }

%% By the ergodic theorem, for each fixed $w$:
%% \cgather{
%% \frac{N_n(w)}{n} \to P(w), \qquad \frac{N_n(w\sigma)}{N_n(w)} \to P(\sigma \mid w) \quad \text{a.s.}
%% }

%% Hence the empirical estimates $\hat{P}_n(w)$ and $\hat{P}_n(\sigma \mid w)$ converge to the true values almost surely.

%% The contribution from rare contexts is:
%% \cgather{
%% R_{n,m} := \frac{1}{n} \sum_{w \notin \mathcal{T}_m^{(n)}} N_n(w) \le \frac{m \cdot |\Sigma|^{s_{\max}}}{n} \to 0.
%% }

%% Therefore, the pruned entropy estimate converges in probability:
%% \cgather{
%% \widehat{H}_n^{(m)} \xrightarrow[n \to \infty]{P} H^\star.
%% }

%% Averaging over $m = 1, \dots, M$ preserves this limit, so:
%% \cgather{
%% \widehat{H}_n \xrightarrow[n \to \infty]{P} H^\star.
%% }

%% \end{proof}

%% \begin{thm}[Finite–Sample Error of the Median--$M$ NERO Estimator]
%% Let $\Sigma$ be a finite alphabet ($|\Sigma|<\infty$) and  
%% let $\{X_t\}_{t\ge1}$ be a stationary ergodic source over~$\Sigma$ with
%% entropy rate $H^\star$.  Fix a maximal context depth $s_{\max}\ge1$ and
%% observe a sample $x_1^n$ of length~$n$.

%% For each threshold $m\ge1$ define the \emph{pruned NERO} estimate
%% \cgather{
%%   \widehat H_n^{(m)}
%%   := -\!\!\!\!\sum_{w:\,N_n(w)>m}\!\!\!\!
%%        \hat P_n(w)
%%        \sum_{\sigma\in\Sigma}
%%          \hat P_n(\sigma\!\mid\!w)\,
%%          \log\hat P_n(\sigma\!\mid\!w),
%% }
%% where  
%% \cgather{
%%   \hat P_n(\sigma\!\mid\!w)=\frac{N_n(w\sigma)}{\sum_{\tau}N_n(w\tau)},
%%   \qquad
%%   \hat P_n(w)=\frac{N_n(w)}{\sum_{v:\,N_n(v)>m}N_n(v)}.
%% }

%% Choose thresholds
%% $m_{\min}\le m_1<\dots<m_M\le m_{\max}$ with
%% $m_{\min}=\Omega(1)$ and $m_{\max}=o(n)$, and form the
%% \emph{median estimator}
%% \cgather{
%%    \widehat H_n :=
%%         \operatorname{median}
%%         \bigl\{\widehat H_n^{(m_1)},\dots,\widehat H_n^{(m_M)}\bigr\}.
%% }

%% \begin{enumerate}
%% \item[\textup{(a)}] \textbf{Mean-squared error.}
%%       \cgather{
%%         \operatorname{MSE}\!\bigl[\widehat H_n\bigr]
%%         =O\!\bigl((|\Sigma|^{\,s_{\max}}/n)^{4/3}\bigr).
%%       }

%% \item[\textup{(b)}] \textbf{Exponential concentration.}
%%       Assume each individual estimate obeys
%%       \cgather{
%%         \Pr\!\bigl(|\widehat H_n^{(m_i)}-H^\star|>\varepsilon\bigr)
%%         \le\delta<\tfrac12,
%%         \qquad i=1,\dots,M.
%%       }
%%       Then
%%       \cgather{
%%         \Pr\!\bigl(|\widehat H_n-H^\star|>\varepsilon\bigr)
%%         \le
%%         \exp\!\bigl[-2M\,(1/2-\delta)^2\bigr].
%%       }
%% \end{enumerate}
%% Hence the median-of-$M$ NERO estimator attains the near-optimal
%% $n^{-4/3}$ rate without threshold tuning, and its tail probability
%% decays exponentially in~$M$.
%% \end{thm}

%% \begin{proof}
%% \emph{Bias.}  Discarded contexts have total mass  
%% \cgather{
%%   \epsilon_{n,m}\;=\;
%%   \sum_{w:N_n(w)\le m}\frac{N_n(w)}{n}
%%   \le
%%   \frac{m\,|\Sigma|^{s_{\max}}}{n}.
%% }
%% Their entropy contribution is at most
%% $\epsilon_{n,m}\log|\Sigma|$, so
%% \cgather{
%%   \text{Bias}_m
%%   =O\!\bigl(m\,|\Sigma|^{s_{\max}}\log|\Sigma|/n\bigr).
%% }

%% \smallskip
%% \noindent
%% \emph{Variance.}  For every kept context $w$ we have $N_n(w)>m$.
%% Multinomial variance plus the $\log|\Sigma|$-Lipschitz property of
%% entropy gives
%% \cgather{
%%   \operatorname{Var}\!\bigl[H(\hat P_n(\cdot\!\mid\!w))\bigr]
%%        =O(\log^2|\Sigma|/m).
%% }
%% With at most $n/m$ such contexts and weights summing to~1,
%% \cgather{
%%   \operatorname{Var}\bigl[\widehat H_n^{(m)}\bigr]
%%   =O\!\bigl(\log^2|\Sigma|/(m n)\bigr).
%% }

%% \smallskip
%% \noindent
%% \emph{Single-$m$ MSE.}
%% \cgather{
%%   \operatorname{MSE}_m
%%   =O\!\Bigl(
%%         \frac{m^2\,|\Sigma|^{2s_{\max}}\log^2|\Sigma|}{n^2}
%%         +\frac{\log^2|\Sigma|}{m n}
%%      \Bigr).
%% }
%% Minimising over $m$ yields
%% $m^\star\asymp (n^2/|\Sigma|^{2s_{\max}})^{1/3}$ and
%% $\operatorname{MSE}_{m^\star}
%%         =O\!\bigl((|\Sigma|^{s_{\max}}/n)^{4/3}\bigr).$

%% \smallskip
%% \noindent
%% \emph{Median.}
%% Any range $[m_{\min},m_{\max}]$ that contains a constant-factor
%% neighbourhood of $m^\star$ guarantees at least half the
%% $\widehat H_n^{(m_i)}$ attain the above rate;
%% removing the worst half therefore preserves that rate,
%% establishing part~(a).

%% For (b), define  
%% \(\,Z_i=\mathbf1\{|\widehat H_n^{(m_i)}-H^\star|>\varepsilon\}\).
%% Then $\sum_i Z_i\sim\mathrm{Binomial}(M,\le\delta)$, and Hoeffding’s
%% inequality gives the stated exponential bound on the event that more
%% than $M/2$ of the $Z_i$ equal~1 (which is exactly
%% \(\{|\,\widehat H_n-H^\star|>\varepsilon\}\)).
%% \end{proof}





\begin{thm}[Robustness and Concentration of the Aggregated Entropy Estimate]\label{thmmedian}
Let \( x_1^n \) be a finite realization from a stationary, ergodic stochastic process over a finite alphabet \( \Sigma \). Let \( \hat{H}^{(m)}_n \) denote the entropy-rate estimate obtained by applying the algorithm of Chattopadhyay $et. al$~\cite{CLx}) with threshold \( m \) (substrings occurring  \( <m \) times ignored). %% Then, 
Then the aggregated estimator defined as:
\[
\hat{H}_n := \mathrm{median} \left\{ \hat{H}^{(m_1)}_n, \hat{H}^{(m_2)}_n, \ldots, \hat{H}^{(m_M)}_n \right\}.
\] satisfies the exponential concentration bound:
\[
\mathbb{P} \left( \left| \hat{H}_n - H_\star \right| > \epsilon \right) \leq \exp \left( - 2 M \left( \frac{1}{2} - \delta \right)^2 \right).
\]
\end{thm}

\begin{proof}
It follows from Chattopadhyay $et. al$~\cite{CLx}): for sufficiently large $n$, for each threshold \( m \) in a chosen set \( \{ m_1, m_2, \ldots, m_M \} \), we have the concentration bound:
\[
\mathbb{P} \left( \left| \hat{H}^{(m)}_n - H_\star \right| > \epsilon \right) \leq \delta,
\]
for some fixed \( \epsilon > 0 \) and \( \delta < \frac{1}{2} \), determined by the algorithm parameters and the data length \( n \).
For each \( m_i \), define the indicator variable:
\[
Z_i = \mathbf{1} \left\{ \left| \hat{H}^{(m_i)}_n - H_\star \right| > \epsilon \right\}.
\]
The concentration bound implies:
\[
\mathbb{E} [ Z_i ] = \mathbb{P} \left( \left| \hat{H}^{(m_i)}_n - H_\star \right| > \epsilon \right) \leq \delta.
\]

The aggregated estimator \( \hat{H}_n \) deviates by more than \( \epsilon \) only if at least half of the individual estimates do, i.e.,
\[
S = \sum_{i=1}^M Z_i \geq \frac{M}{2}.
\]

Applying Hoeffding's inequality for bounded independent random variables \( Z_1, \ldots, Z_M \) yields:
\[
\mathbb{P} \left( S \geq \frac{M}{2} \right) \leq \exp \left( - 2 M \left( \frac{1}{2} - \delta \right)^2 \right).
\]

Thus, the aggregated median estimator satisfies:
\[
\mathbb{P} \left( \left| \hat{H}_n - H_\star \right| > \epsilon \right) \leq \exp \left( - 2 M \left( \frac{1}{2} - \delta \right)^2 \right),
\]
completing the proof.
\end{proof}







